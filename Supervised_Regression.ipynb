{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from utils_data import * \n",
    "from utils_ml import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotprediction_TS(test_dates, final_predictions, test_labels):\n",
    "    import seaborn as sns\n",
    "    df_to_compare = pd.DataFrame({'date': test_dates, 'Actual': test_labels, 'Predicted': final_predictions})\n",
    "    dfm = pd.melt(df_to_compare, id_vars=['date'], value_vars=['Actual', 'Predicted'], var_name='data', value_name='precip')\n",
    "    f, axs = plt.subplots(1,2,\n",
    "                      figsize=(12,5),\n",
    "                      sharey=True)\n",
    "\n",
    "    sns.regplot(data= df_to_compare,\n",
    "                x=\"Actual\",\n",
    "                y=\"Predicted\",\n",
    "                ax=axs[0],\n",
    "                )\n",
    "    sns.lineplot(x='date', y='precip', hue = 'data', data=dfm, ax=axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "os.getcwd() \n",
    "DATADIR = '/Users/noeliaotero/Documents/CAS_ML/data/'\n",
    "\n",
    "# Some constants\n",
    "DATE_START = '1979-01-01'\n",
    "DATE_END = '2020-12-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRCSV  = DATADIR + 'TS_CH/'\n",
    "l_files = glob.glob(os.path.join(DIRCSV, 'df*.csv'))\n",
    "df_vars =  read_csv_files(l_files, DATE_START, DATE_END)\n",
    "df_vars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lag-t2mmean\n",
    "df_vars['T2MLag'] = df_vars['T2MMEAN'].shift(1)\n",
    "df_vars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read precipitaion\n",
    "df_prec = get_precipitation_data ( DATADIR + 'TS_CH/precip_regions.csv', DATE_START, DATE_END)\n",
    "# Select the right columns: date and reg_tot (all country)\n",
    "df_prec = df_prec[['date','reg_tot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Large scale-atmospheric PCs and Clusters\n",
    "df_PCs = pd.read_csv(DATADIR + 'ERA5/PCdf.csv')\n",
    "df_PCs['date'] = pd.DatetimeIndex(df_PCs['date']).normalize()\n",
    "df_clusters =  pd.read_csv(DATADIR + 'ERA5/Cluster_spatialmean.csv')\n",
    "df_clus    = df_clusters[['date','Cluster']]\n",
    "df_clus['date'] = pd.DatetimeIndex(df_clus['date']).normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge everything\n",
    "l_all = []\n",
    "l_all.append(df_vars)\n",
    "l_all.append(df_prec)\n",
    "df_all = concat_dataframes(l_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "seaborn.pairplot(df_all, vars=df_all.columns[1:9],\n",
    "                 kind='reg')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas.tools.plotting import scatter_matrix # For older versions of Pandas\n",
    "#from pandas.plotting import scatter_matrix\n",
    "\n",
    "#scatter_matrix(df_input[df_input.columns[1:9]], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy_train = [1979,2015]\n",
    "yy_test  = [2016,2020]\n",
    "ylabel = df_prec.columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add categorical variables\n",
    "#df_input = pd.merge(df_all, df_clus)\n",
    "df_input = pd.merge(df_all, df_PCs)\n",
    "names_col = df_input.columns\n",
    "# define attributes - i.e covariates\n",
    "attributes = names_col.drop(['date','reg_tot'])\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Time series\n",
    "from matplotlib import pyplot\n",
    "df_input['reg_tot'].plot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_labels, test_dataset, test_labels, train_dates, test_dates = split_data(df_input, yy_train, yy_test, attributes, ylabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_var='Cluster'\n",
    "fpipeline = prepareData(train_dataset, None)\n",
    "X_prep_train = fpipeline.fit_transform(train_dataset)\n",
    "X_prep_test = fpipeline.fit_transform(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prep_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libraries\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(n_jobs=16)\n",
    "lr.fit(X_prep_train, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train = mean_squared_error(train_labels, lr.predict(X_prep_train))\n",
    "mse_test = mean_squared_error(test_labels, lr.predict(X_prep_test))\n",
    "print(f'Train MSE = {mse_train}'); print(f'Test MSE = {mse_test}')\n",
    "print(f'Train RMSE = {np.sqrt(mse_train)}'); print(f'Test RMSE = {np.sqrt(mse_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we want to apply model selection?\n",
    "RFE (Recursive feature elimination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(lr)             \n",
    "rfe = rfe.fit(X_prep_train, train_labels)\n",
    "mean_squared_error(train_labels, rfe.predict(X_prep_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(x, y):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(test_labels, preds)\n",
    "    ax.plot([test_labels.min(), test_labels.max()], [test_labels.min(), test_labels.max()], 'k--', lw=1)\n",
    "    ax.set_xlabel('Measured')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_val_predict returns an array of the same size as `y` where each entry\n",
    "# is a prediction obtained by cross validated:\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cv_mse = cross_val_score(lr, X_prep_train, train_labels, scoring='neg_mean_squared_error', cv=10)\n",
    "# We got the negative average MSE for cross-validation (minimizing MSE is equivalent to maximizing the negative MSE)\n",
    "lr_cv_mse.mean()\n",
    "# The result is close to what we obtained before. The negative result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_rmse_scores = np.sqrt(-lr_cv_mse)\n",
    "pd.Series(lin_rmse_scores).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Folds: \" + str(len(lr_cv_mse)) + \", MSE: \" + str(np.mean(np.abs(lr_cv_mse))) + \", STD: \" + str(np.std(lr_cv_mse)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the coefficients\n",
    "lr.coef_\n",
    "#coeff_df = pd.DataFrame(lr.coef_, attributes, columns=['Coefficient'])\n",
    "# makes some predictions\n",
    "y_pred = lr.predict(X_prep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotprediction_TS(test_dates, y_pred, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "forest_reg.fit(X_prep_train, train_labels)\n",
    "# make predictions\n",
    "y_rf_pred = forest_reg.predict(X_prep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_rf_train = mean_squared_error(train_labels, forest_reg.predict(X_prep_train))\n",
    "mse_rf_test = mean_squared_error(test_labels, forest_reg.predict(X_prep_test))\n",
    "print(f'Train MSE = {mse_rf_train}'); print(f'Test MSE = {mse_rf_test}')\n",
    "print(f'Train RMSE = {np.sqrt(mse_rf_train)}'); print(f'Test RMSE = {np.sqrt(mse_rf_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it can be noted overfitting problem, the RMSE is much higher for the test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tunning parameter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    # try 12 (3×4) combinations of hyperparameters\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6]},\n",
    "    # then try 6 (2×3) combinations with bootstrap set as False\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "# Create the parameter grid based on the results of random search \n",
    "    \n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(X_prep_train, train_labels)\n",
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Best Score:\" , grid_search.best_score_)\n",
    "print (\"Best params:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_GCV_reg = RandomForestRegressor(n_jobs=-1).set_params(**best_params)\n",
    "forest_GCV_reg.fit(X_prep_train,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rf_cv_predict = forest_GCV_reg.predict(X_prep_test)\n",
    "mse_rf_cv_train = mean_squared_error(train_labels, forest_GCV_reg.predict(X_prep_train))\n",
    "mse_rf_cv_test = mean_squared_error(test_labels, forest_GCV_reg.predict(X_prep_test))\n",
    "print(f'Train MSE = {mse_rf_cv_train}'); print(f'Test MSE = {mse_rf_cv_test}')\n",
    "print(f'Train RMSE = {np.sqrt(mse_rf_cv_train)}'); print(f'Test RMSE = {np.sqrt(mse_rf_cv_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotprediction_TS(test_dates, y_rf_cv_predict, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importance = forest_GCV_reg.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_features_importance = sorted(zip(features_importance, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_features_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting extremes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prec_ex = precip_exceedance(df_prec)\n",
    "df_prec_ex['reg_tot'] = df_prec_ex['reg_tot']*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_ex = df_input\n",
    "# Replace reg_tot by the exceedances\n",
    "df_input_ex['reg_tot'] = df_prec_ex['reg_tot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_labels, test_dataset, test_labels, train_dates, test_dates = split_data(df_input_ex, yy_train, yy_test, attributes, ylabel)\n",
    "# but the data is already in the format (only the labels have been replace by the exceedances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# all parameters not specified are set to their defaults\n",
    "logisticRegr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "logisticRegr.fit(X_prep_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ex_pred=logisticRegr.predict(X_prep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the metrics class\n",
    "from sklearn import metrics\n",
    "cnf_matrix = metrics.confusion_matrix(test_labels, y_ex_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
