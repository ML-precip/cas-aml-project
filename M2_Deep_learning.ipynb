{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "\n",
    "Import necessary modules and do some basic setup."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "from tensorflow import keras\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Config matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Custom utils\n",
    "from utils_data import *\n",
    "from utils_ml import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define some paths and constants."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Paths\n",
    "DATADIR = os.getcwd() + '/../data'\n",
    "\n",
    "# Some constants\n",
    "DATE_START = '1979-01-01'\n",
    "DATE_END = '2020-12-31'\n",
    "YY_TRAIN = [1979, 2015]\n",
    "YY_TEST = [2016, 2020]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "## Getting started with the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Dataset**: RhiresD, which is a gridded daily precipitation dataset over Switzerland provided by MeteoSwiss. It is based on a spatial interpolation of rain-gauge data. The grid resolution is 1 km, but the effective resolution is in the order of 15-20 km.\n",
    "\n",
    "\n",
    "**Aggregations levels**: The gridded dataset has been averaged over different regions:\n",
    "* 12 climatic regions\n",
    "* 5 aggregated regions\n",
    "* the whole country"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Read precipitation file\n",
    "df_prec = get_precipitation_data(DATADIR + '/MeteoSwiss/precip_regions.csv',\n",
    "                                 DATE_START, DATE_END)\n",
    "\n",
    "df_prec = prepare_prec_data_by_aggregated_regions(df_prec, qt=0.95)\n",
    "prec_cols = ['reg_1', 'reg_2', 'reg_3', 'reg_4', 'reg_5', 'reg_tot']\n",
    "prec_xtr_cols = ['reg_1_xtr', 'reg_2_xtr', 'reg_3_xtr', 'reg_4_xtr', 'reg_5_xtr', 'reg_tot_xtr']\n",
    "\n",
    "df_prec.describe(exclude='datetime')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Read predictors file\n",
    "l_files = glob.glob(os.path.join(DATADIR + '/ERA5/TS_CH/', 'df*.csv'))\n",
    "df_vars = read_csv_files(l_files, DATE_START, DATE_END)\n",
    "\n",
    "df_vars.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot some data\n",
    "fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(20, 10))\n",
    "df_vars.Z500.plot.hist(ax=axs[0, 0], bins=20)\n",
    "axs[0, 0].set_title('Geopotentiel 500hPa')\n",
    "df_vars.MSL.plot.hist(ax=axs[1, 0], bins=20)\n",
    "axs[1, 0].set_title('Sea level pressure')\n",
    "df_vars.T2MMEAN.plot.hist(ax=axs[0, 1], bins=20)\n",
    "axs[0, 1].set_title('Temperature 2m')\n",
    "df_prec.reg_tot.plot.hist(ax=axs[1, 1], bins=20)\n",
    "axs[1, 1].set_title('Precipitation')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using time series of mean variable values over Switzerland as predictors\n",
    "\n",
    "Objective: compare with previous analyses."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Split set into training and testing based on dates\n",
    "X_train_full = df_vars[(df_vars.date.dt.year >= YY_TRAIN[0]) &\n",
    "                       (df_vars.date.dt.year <= YY_TRAIN[1])]\n",
    "X_test = df_vars[(df_vars.date.dt.year >= YY_TEST[0]) &\n",
    "                 (df_vars.date.dt.year <= YY_TEST[1])]\n",
    "y_train_full = df_prec[(df_prec.date.dt.year >= YY_TRAIN[0]) &\n",
    "                       (df_prec.date.dt.year <= YY_TRAIN[1])]\n",
    "y_test = df_prec[(df_prec.date.dt.year >= YY_TEST[0]) &\n",
    "                 (df_prec.date.dt.year <= YY_TEST[1])]\n",
    "\n",
    "# Drop dates\n",
    "X_train_full = X_train_full.drop(columns=['date'])\n",
    "X_test = X_test.drop(columns=['date'])\n",
    "\n",
    "# Split full training into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Transform data\n",
    "num_attribs = list(X_train)\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "])\n",
    "\n",
    "X_train = full_pipeline.fit_transform(X_train)\n",
    "X_valid = full_pipeline.transform(X_valid)\n",
    "X_test = full_pipeline.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prediction of precipitation values over Switzerland (overall mean)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Clear session and set tf seed\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ANN using timeseries to predict precipitation\n",
    "input_dim = X_train.shape[1]\n",
    "ann_prec_v1 = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_dim=input_dim),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"relu\")\n",
    "])\n",
    "\n",
    "ann_prec_v1.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ann_prec_v1.compile(loss=\"mse\",\n",
    "                    optimizer=\"adam\")\n",
    "\n",
    "history = ann_prec_v1.fit(X_train, y_train.reg_tot, epochs=30,\n",
    "                          validation_data=(X_valid, y_valid.reg_tot))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.set_ylim(0, 1.05*max(max(history.history['loss']), max(history.history['val_loss'])))\n",
    "ax.plot(history.history['loss'], label='loss', color='C0')\n",
    "ax.plot(history.history['val_loss'], label='val_loss', color='C1')\n",
    "plt.grid(True)\n",
    "fig.legend(bbox_to_anchor=(1.0,0.5), loc=\"center left\", borderaxespad=0)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ann_prec_v1.evaluate(X_test, y_test.reg_tot)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Summary:**\n",
    "* Tested with different structures, does not change the skill\n",
    "* Other hyperparameters not likely to save the day\n",
    "* About the same skill as random forest\n",
    "\n",
    "**Conclusion:** Not satisfying (to not say BS). These predictors are not able to predict precipitation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prediction of precipitation extremes over Switzerland (overall mean)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Clear session and set tf seed\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ANN using timeseries to predict precipitation\n",
    "input_dim = X_train.shape[1]\n",
    "ann_prec_v2 = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_dim=input_dim),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "ann_prec_v2.compile(loss=\"binary_crossentropy\",\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=[\"binary_accuracy\"])\n",
    "\n",
    "history = ann_prec_v2.fit(X_train, y_train.reg_tot_xtr, epochs=20,\n",
    "                          validation_data=(X_valid, y_valid.reg_tot_xtr))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "ax2 = ax1.twinx()\n",
    "ax1.set_ylim(0, 1.05*max(max(history.history['loss']), max(history.history['val_loss'])))\n",
    "ax2.set_ylim(0, 1)\n",
    "ax1.plot(history.history['loss'], label='loss', color='C0')\n",
    "ax1.plot(history.history['val_loss'], label='val_loss', color='C1')\n",
    "ax2.plot(history.history['binary_accuracy'], label='binary_accuracy', color='C2')\n",
    "ax2.plot(history.history['val_binary_accuracy'], label='val_binary_accuracy', color='C3')\n",
    "plt.grid(True)\n",
    "fig.legend(bbox_to_anchor=(1.0,0.5), loc=\"center left\", borderaxespad=0)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using gridded data over a larger domain as predictors\n",
    "\n",
    "Objective: get some spatial information"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Grid options (total extent: 80° lon & 50° lon)\n",
    "step_lat=1\n",
    "step_lon=1\n",
    "nb_lat=21\n",
    "nb_lon=31"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load gridded data\n",
    "ds_z = get_era5_data(DATADIR + '/ERA5/geopotential/*.nc', DATE_START, DATE_END)\n",
    "z = extract_points_around_CH(ds_z, step_lat=step_lat, step_lon=step_lon, nb_lat=nb_lat, nb_lon=nb_lon, levels=[300, 500, 700, 850, 1000])\n",
    "ds_t2m = get_era5_data(DATADIR + '/ERA5/Daymean_era5_T2M_EU_19790101-20210905.nc', DATE_START, DATE_END)\n",
    "t2m = extract_points_around_CH(ds_t2m, step_lat=step_lat, step_lon=step_lon, nb_lat=nb_lat, nb_lon=nb_lon)\n",
    "t2m['time'] = pd.DatetimeIndex(t2m.time.dt.date)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We have arrays of 2D fields ...\n",
    "t2m.dims"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ... as well as arrays of 3D fields (with pressure levels)\n",
    "z.dims"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Split set into training and testing based on dates\n",
    "X1_train_full = z.sel(time=slice(\"{}-01-01\".format(YY_TRAIN[0]), \"{}-12-31\".format(YY_TRAIN[1])))\n",
    "X1_test = z.sel(time=slice(\"{}-01-01\".format(YY_TEST[0]), \"{}-12-31\".format(YY_TEST[1])))\n",
    "X2_train_full = t2m.sel(time=slice(\"{}-01-01\".format(YY_TRAIN[0]), \"{}-12-31\".format(YY_TRAIN[1])))\n",
    "X2_test = t2m.sel(time=slice(\"{}-01-01\".format(YY_TEST[0]), \"{}-12-31\".format(YY_TEST[1])))\n",
    "\n",
    "# Stack arrays (reduce dimensions to 2D arrays)\n",
    "X1_train_full = X1_train_full.to_stacked_array(\"xyz\", sample_dims=[\"time\"])\n",
    "X1_test = X1_test.to_stacked_array(\"xyz\", sample_dims=[\"time\"])\n",
    "X2_train_full = X2_train_full.expand_dims(\"level\")\n",
    "X2_train_full = X2_train_full.to_stacked_array(\"xyz\", sample_dims=[\"time\"])\n",
    "X2_test = X2_test.expand_dims(\"level\")\n",
    "X2_test = X2_test.to_stacked_array(\"xyz\", sample_dims=[\"time\"])\n",
    "\n",
    "# Split full training into training and validation sets\n",
    "X1_train, X1_valid, X2_train, X2_valid, y_train, y_valid = train_test_split(X1_train_full, X2_train_full, y_train_full, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Transform to pandas dataframe\n",
    "X1_train = X1_train.to_pandas()\n",
    "X1_valid = X1_valid.to_pandas()\n",
    "X1_test = X1_test.to_pandas()\n",
    "X2_train = X2_train.to_pandas()\n",
    "X2_valid = X2_valid.to_pandas()\n",
    "X2_test = X2_test.to_pandas()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Merge arrays\n",
    "X_train = pd.merge(X1_train, X2_train, how='outer', on='time')\n",
    "X_valid = pd.merge(X1_valid, X2_valid, how='outer', on='time')\n",
    "X_test = pd.merge(X1_test, X2_test, how='outer', on='time')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Transform data\n",
    "num_attribs = X_train.columns\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "])\n",
    "\n",
    "X_train = full_pipeline.fit_transform(X_train)\n",
    "X_valid = full_pipeline.transform(X_valid)\n",
    "X_test = full_pipeline.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prediction of precipitation values over Switzerland (overall mean)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Clear session and set tf seed\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ANN using gridded data to predict precipitation\n",
    "input_dim = X_train.shape[1]\n",
    "ann_prec_v3 = keras.models.Sequential([\n",
    "    keras.layers.Dense(300, activation=\"relu\", input_dim=input_dim),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"relu\")\n",
    "])\n",
    "\n",
    "ann_prec_v3.compile(loss=\"mse\",\n",
    "                    optimizer=\"adam\")\n",
    "\n",
    "history = ann_prec_v3.fit(X_train, y_train.reg_tot, epochs=20,\n",
    "                          validation_data=(X_valid, y_valid.reg_tot))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.set_ylim(0, 1.05*max(max(history.history['loss']), max(history.history['val_loss'])))\n",
    "ax.plot(history.history['loss'], label='loss', color='C0')\n",
    "ax.plot(history.history['val_loss'], label='val_loss', color='C1')\n",
    "plt.grid(True)\n",
    "fig.legend(bbox_to_anchor=(1.0,0.5), loc=\"center left\", borderaxespad=0)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ann_prec_v3.evaluate(X_test, y_test.reg_tot)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extreme events"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Clear session and set tf seed\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ANN using timeseries to predict precipitation\n",
    "input_dim = X_train.shape[1]\n",
    "ann_xtrm_v2 = keras.models.Sequential([\n",
    "    keras.layers.Dense(300, activation=\"relu\", input_dim=input_dim),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "ann_xtrm_v2.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ann_xtrm_v2.compile(loss=\"binary_crossentropy\",\n",
    "                    optimizer=\"adam\")\n",
    "\n",
    "history = ann_xtrm_v2.fit(X_train, y_train, epochs=30,\n",
    "                          validation_data=(X_valid, y_valid))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "------------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.evaluate(X_test, y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ANN using timeseries to predict extreme events\n",
    "ann_ts_precip = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(2, activation=\"tanh\") # try with sigmoid to have the probability !\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using gridded data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ANN using gridded data to predict precipitation\n",
    "ann_ts_precip = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# ANN using gridded data to predict extreme events\n",
    "ann_ts_precip = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction of extreme events"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "58697530e16d0976ccf54d6bb39956972008ad73061b865335f8b29c4298d2df"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}