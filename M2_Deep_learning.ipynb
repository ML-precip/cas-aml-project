{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Import necessary modules and do some basic setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= '0.20'\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ >= '2.0'\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Input, Dropout, MaxPooling2D, Flatten\n",
    "import tensorflow.keras.backend as tfb\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dask\n",
    "import math\n",
    "dask.config.set({'array.slicing.split_large_chunks': False})\n",
    "from functools import partial\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Config matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Custom utils\n",
    "from utils_data import *\n",
    "from utils_ml import *\n",
    "from utils_plot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some paths and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATADIR = os.path.join(os.getcwd(), '..', 'data')\n",
    "\n",
    "# Some constants\n",
    "DATE_START = '1979-01-01'\n",
    "DATE_END = '2020-12-31'\n",
    "YY_TRAIN = [1979, 2015]\n",
    "YY_TEST = [2016, 2020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing precipitation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**: RhiresD, which is a gridded daily precipitation dataset over Switzerland provided by MeteoSwiss. It is based on a spatial interpolation of rain-gauge data. The grid resolution is 1 km, but the effective resolution is in the order of 15-20 km.\n",
    "\n",
    "\n",
    "**Aggregations levels**: The gridded dataset has been averaged over different regions:\n",
    "* 12 climatic regions\n",
    "* 5 aggregated regions\n",
    "* the whole country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read precipitation file\n",
    "df_prec = get_precipitation_data(DATADIR + '/MeteoSwiss/precip_regions.csv',\n",
    "                                 DATE_START, DATE_END)\n",
    "\n",
    "df_prec = prepare_prec_data_by_aggregated_regions(df_prec, qt=0.95)\n",
    "prec_cols = df_prec.columns[1:7]\n",
    "prec_xtr_cols = df_prec.columns[7:13]\n",
    "\n",
    "df_prec.describe(exclude='datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select regions of interest for following analyses (for example only 'reg_tot' or all sub-regions)\n",
    "regions = ['reg_1', 'reg_2', 'reg_3', 'reg_4', 'reg_5', 'reg_tot']\n",
    "regions_xtr = ['reg_1_xtr', 'reg_2_xtr', 'reg_3_xtr', 'reg_4_xtr', 'reg_5_xtr', 'reg_tot_xtr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 1: ANN - Using time series of mean variable values over Switzerland as input\n",
    "\n",
    "Objective: compare with previous analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing input data (predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input files\n",
    "l_files = glob.glob(os.path.join(DATADIR, 'ERA5', 'TS_CH', 'regions', 'df*.csv'))\n",
    "l_files.append(os.path.join(DATADIR, 'ERA5', 'PCdf.csv'))\n",
    "df_vars = read_csv_files(l_files, DATE_START, DATE_END, rename_columns=True)\n",
    "df_vars = df_vars.drop(columns=list(df_vars.filter(regex='MSL')))\n",
    "\n",
    "df_vars.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split set into training and testing based on dates\n",
    "X_train_full = df_vars[(df_vars.date.dt.year >= YY_TRAIN[0]) &\n",
    "                       (df_vars.date.dt.year <= YY_TRAIN[1])]\n",
    "X_test = df_vars[(df_vars.date.dt.year >= YY_TEST[0]) &\n",
    "                 (df_vars.date.dt.year <= YY_TEST[1])]\n",
    "y_train_full = df_prec[(df_prec.date.dt.year >= YY_TRAIN[0]) &\n",
    "                       (df_prec.date.dt.year <= YY_TRAIN[1])]\n",
    "y_test = df_prec[(df_prec.date.dt.year >= YY_TEST[0]) &\n",
    "                 (df_prec.date.dt.year <= YY_TEST[1])]\n",
    "\n",
    "# Drop dates column\n",
    "X_train_full = X_train_full.drop(columns=['date'])\n",
    "X_test = X_test.drop(columns=['date'])\n",
    "\n",
    "# Split full training into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data\n",
    "num_attribs = list(X_train)\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_attribs),\n",
    "])\n",
    "\n",
    "X_train = full_pipeline.fit_transform(X_train)\n",
    "X_valid = full_pipeline.transform(X_valid)\n",
    "X_test = full_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of precipitation **values** over Switzerland (overall mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear session and set tf seed\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ANN using timeseries to predict precipitation\n",
    "input_dim = X_train.shape[1]\n",
    "ann_prec_v1 = keras.models.Sequential([\n",
    "    Input(shape=input_dim),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(len(regions), activation='relu')\n",
    "])\n",
    "\n",
    "ann_prec_v1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model and train\n",
    "ann_prec_v1.compile(loss='mse',\n",
    "                    optimizer='adam')\n",
    "\n",
    "history = ann_prec_v1.fit(X_train, y_train[regions], epochs=50,\n",
    "                          validation_data=(X_valid, y_valid[regions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training evolution\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1.05*max(max(history.history['loss']), max(history.history['val_loss'])))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average score\n",
    "test_mse = ann_prec_v1.evaluate(X_test, y_test[regions])\n",
    "print(f'Test average MSE: {test_mse:.2f}')\n",
    "print(f'Test average RMSE: {math.sqrt(test_mse):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores per region\n",
    "y_pred = ann_prec_v1.predict(X_test)\n",
    "scores = np.sqrt(np.square(np.subtract(y_test[regions], y_pred)).mean())\n",
    "scores.name = 'RMSE'\n",
    "print(scores.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of the predictions vs observations\n",
    "plot_prediction_scatter(y_test[regions], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "* Tested with different structures, does not change the skill\n",
    "* Other hyperparameters not likely to save the day\n",
    "* About the same skill as random forest\n",
    "\n",
    "**Conclusion:** Not satisfying. These inputs are not able to predict precipitation well enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of precipitation **extremes** over Switzerland (overall mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear session and set tf seed\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ANN using timeseries to predict precipitation\n",
    "input_dim = X_train.shape[1]\n",
    "ann_xtrm_v1 = keras.models.Sequential([\n",
    "    Input(shape=input_dim),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(len(regions_xtr), activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model and train\n",
    "ann_xtrm_v1.compile(\n",
    "    optimizer='adam',\n",
    "    loss=WeightedBinaryCrossEntropy(\n",
    "        pos_weight=10,\n",
    "        weight=1,\n",
    "        from_logits=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "#ann_xtrm_v1.compile(\n",
    "#    optimizer='adam',\n",
    "#    loss=weighted_binary_crossentropy\n",
    "#)\n",
    "\n",
    "history = ann_xtrm_v1.fit(X_train, y_train[regions_xtr].astype(float), epochs=50,\n",
    "                          validation_data=(X_valid, y_valid[regions_xtr].astype(float)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate the extremes\n",
    "y_pred_train = ann_xtrm_v1.predict(X_train)\n",
    "y_pred_test = ann_xtrm_v1.predict(X_test)\n",
    "\n",
    "y_pred_train_bool = y_pred_train >= 0.5\n",
    "y_pred_test_bool = y_pred_test >= 0.5\n",
    "\n",
    "# Confusion matrix per region (x: prediction; y: true value)\n",
    "# [[TN, FP] \n",
    "# [ FN, TP]]\n",
    "for idx, region in enumerate(regions_xtr):\n",
    "    cnf_matrix = confusion_matrix(y_test[region], y_pred_test_bool[:, idx])\n",
    "    print(f\"Confusion matrix {region}:\\n {cnf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for the whole country\n",
    "\n",
    "# ROC, AUC, Recall, Precision\n",
    "# Recall = TruePositives / (TruePositives + FalseNegatives); best = 1\n",
    "# Precision = TruePositives / (TruePositives + FalsePositives); best = 1\n",
    "evaluate_model(y_test.reg_tot_xtr, y_train.reg_tot_xtr, y_pred_test_bool[:, -1],\n",
    "                y_pred_test[:, -1], y_pred_train_bool[:, -1], y_pred_train[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Analysis 2: ANN - Using gridded data over a larger domain as input\n",
    "\n",
    "Objective: get some spatial information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing input data (predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid options (total extent: 80° lon & 50° lat)\n",
    "resolution = 1\n",
    "nb_lat = 20 * 1/resolution + 1\n",
    "nb_lon = 30 * 1/resolution + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gridded data\n",
    "ds_z = get_era5_data(DATADIR + '/ERA5/geopotential/*.nc', DATE_START, DATE_END)\n",
    "z = extract_points_around_CH(ds_z, step_lat=resolution, step_lon=resolution, nb_lat=nb_lat, nb_lon=nb_lon, levels=[300, 500, 700, 850, 1000])\n",
    "ds_t2m = get_era5_data(DATADIR + '/ERA5/Daymean_era5_T2M_EU_19790101-20210905.nc', DATE_START, DATE_END)\n",
    "t2m = extract_points_around_CH(ds_t2m, step_lat=resolution, step_lon=resolution, nb_lat=nb_lat, nb_lon=nb_lon)\n",
    "t2m['time'] = pd.DatetimeIndex(t2m.time.dt.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have arrays of 2D fields ...\n",
    "t2m.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... as well as arrays of 3D fields (with pressure levels)\n",
    "z.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a level dimension to the 2D dataset\n",
    "t2m = t2m.expand_dims('level', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split set into (training + validation) and testing based on dates\n",
    "z_train_full = z.sel(time=slice('{}-01-01'.format(YY_TRAIN[0]), '{}-12-31'.format(YY_TRAIN[1])))\n",
    "z_test = z.sel(time=slice('{}-01-01'.format(YY_TEST[0]), '{}-12-31'.format(YY_TEST[1])))\n",
    "t2m_train_full = t2m.sel(time=slice('{}-01-01'.format(YY_TRAIN[0]), '{}-12-31'.format(YY_TRAIN[1])))\n",
    "t2m_test = t2m.sel(time=slice('{}-01-01'.format(YY_TEST[0]), '{}-12-31'.format(YY_TEST[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to numpy arrays and concatenate (takes time as it needs to load data from files)\n",
    "X_train_full = np.concatenate((np.squeeze(z_train_full.to_array().to_numpy(), axis=0),\n",
    "                               np.squeeze(t2m_train_full.to_array().to_numpy(), axis=0)), axis=1)\n",
    "X_test = np.concatenate((np.squeeze(z_test.to_array().to_numpy(), axis=0),\n",
    "                         np.squeeze(t2m_test.to_array().to_numpy(), axis=0)), axis=1)\n",
    "\n",
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split full training into training and validation sets (and shuffle)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape arrays to 2D arrays for transformation\n",
    "shape = X_train.shape\n",
    "X_train_flat = X_train.reshape((X_train.shape[0], shape[1]*shape[2]*shape[3]))\n",
    "X_valid_flat = X_valid.reshape((X_valid.shape[0], shape[1]*shape[2]*shape[3]))\n",
    "X_test_flat = X_test.reshape((X_test.shape[0], shape[1]*shape[2]*shape[3]))\n",
    "\n",
    "X_train_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data\n",
    "num_attribs = X_train_flat.shape[1]\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, list(range(num_attribs))),\n",
    "])\n",
    "\n",
    "X_train_flat = full_pipeline.fit_transform(X_train_flat)\n",
    "X_valid_flat = full_pipeline.transform(X_valid_flat)\n",
    "X_test_flat = full_pipeline.transform(X_test_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of precipitation **values** over Switzerland (overall mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear session and set tf seed\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ANN options\n",
    "dropout_rate = 0.1\n",
    "n_dense_neurons = 64/(1-dropout_rate)\n",
    "\n",
    "# ANN using gridded data to predict precipitation\n",
    "input_dim = X_train_flat.shape[1]\n",
    "ann_prec_v2 = keras.models.Sequential([\n",
    "    Input(shape=input_dim),\n",
    "    Dense(n_dense_neurons, activation='selu', kernel_initializer='he_normal'),\n",
    "    Dropout(dropout_rate),\n",
    "    Dense(n_dense_neurons, activation='selu', kernel_initializer='he_normal'),\n",
    "    Dropout(dropout_rate),\n",
    "    # last: relu to avoid negative values\n",
    "    Dense(len(regions), activation='relu')\n",
    "])\n",
    "\n",
    "ann_prec_v2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model and train\n",
    "ann_prec_v2.compile(loss='mse',\n",
    "                    optimizer='adam')\n",
    "\n",
    "history = ann_prec_v2.fit(X_train_flat, y_train[regions], epochs=50,\n",
    "                          validation_data=(X_valid_flat, y_valid[regions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training evolution\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1.05*max(max(history.history['loss']), max(history.history['val_loss'])))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores per region\n",
    "y_pred = ann_prec_v2.predict(X_test_flat)\n",
    "scores = np.sqrt(np.square(np.subtract(y_test[regions], y_pred)).mean())\n",
    "scores.name = 'RMSE'\n",
    "print(scores.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of the predictions vs observations\n",
    "plot_prediction_scatter(y_test[regions], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of precipitation **extremes** over Switzerland (overall mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear session and set tf seed\n",
    "from sklearn.utils import class_weight\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ANN options\n",
    "dropout_rate = 0.1\n",
    "n_dense_neurons = 128/(1-dropout_rate)\n",
    "\n",
    "# ANN using gridded data to predict precipitation extremes\n",
    "input_dim = X_train_flat.shape[1]\n",
    "ann_xtrm_v2 = keras.models.Sequential([\n",
    "    Input(shape=input_dim),\n",
    "    Dense(n_dense_neurons, activation='selu', kernel_initializer='he_normal'),\n",
    "    Dropout(dropout_rate),\n",
    "    Dense(n_dense_neurons, activation='selu', kernel_initializer='he_normal'),\n",
    "    Dropout(dropout_rate),\n",
    "    Dense(len(regions_xtr), activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model and train\n",
    "ann_xtrm_v2.compile(loss=tf.nn.weighted_cross_entropy_with_logits,#(pos_weight=20),\n",
    "                    optimizer='adam',\n",
    "                    metrics=['binary_accuracy'])\n",
    "\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  classes=[False, True],\n",
    "                                                  y=y_train.reg_tot_xtr)\n",
    "\n",
    "history = ann_xtrm_v2.fit(X_train_flat, y_train[regions_xtr], epochs=50,\n",
    "                          validation_data=(X_valid_flat, y_valid[regions_xtr]),\n",
    "                          class_weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate the extremes\n",
    "y_pred_train = ann_xtrm_v2.predict(X_train_flat)\n",
    "y_pred_test = ann_xtrm_v2.predict(X_test_flat)\n",
    "\n",
    "y_pred_train_bool = y_pred_train >= 0.5\n",
    "y_pred_test_bool = y_pred_test >= 0.5\n",
    "\n",
    "# Confusion matrix per region\n",
    "for idx, region in enumerate(regions_xtr):\n",
    "    cnf_matrix = confusion_matrix(y_test[region], y_pred_test_bool[:, idx])\n",
    "    print(f\"Confusion matrix {region}:\\n {cnf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for the whole country\n",
    "evaluate_model(y_test.reg_tot_xtr, y_train.reg_tot_xtr, y_pred_test_bool[:, -1],\n",
    "                y_pred_test[:, -1], y_pred_train_bool[:, -1], y_pred_train[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Analysis 3: CNN - Using gridded data as input\n",
    "\n",
    "Objective: better use spatial information\n",
    "\n",
    "Data: same as previous analysis, but not flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing input data (predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "X_mean = X_train.mean(axis=0, keepdims=True)\n",
    "X_std = X_train.std(axis=0, keepdims=True)\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "X_valid = (X_valid - X_mean) / X_std\n",
    "X_test = (X_test - X_mean) / X_std\n",
    "\n",
    "# Reshape data (set channel first; Con2D option data_format='channels_first' does not work on Win 10 64 bit)\n",
    "X_train = np.moveaxis(X_train, 1, -1)\n",
    "X_valid = np.moveaxis(X_valid, 1, -1)\n",
    "X_test = np.moveaxis(X_test, 1, -1)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of precipitation **values** over Switzerland (overall mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear session and set tf seed\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# CNN using gridded data to predict precipitation\n",
    "input_shape = X_train.shape[1:]\n",
    "cnn_prec_v1 = keras.models.Sequential([\n",
    "    Input(shape=input_shape),\n",
    "    Conv2D(64, 7, padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='relu'),\n",
    "])\n",
    "\n",
    "cnn_prec_v1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model and train\n",
    "cnn_prec_v1.compile(loss='mse',\n",
    "                    optimizer='adam')\n",
    "\n",
    "history = cnn_prec_v1.fit(X_train, y_train.reg_tot, epochs=30,\n",
    "                          validation_data=(X_valid, y_valid.reg_tot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training evolution\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1.05*max(max(history.history['loss']), max(history.history['val_loss'])))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate the extremes\n",
    "y_pred = cnn_prec_v1.predict(X_test)\n",
    "scores = np.sqrt(np.square(np.subtract(y_test[regions], y_pred)).mean())\n",
    "scores.name = 'RMSE'\n",
    "print(scores.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of the predictions vs observations\n",
    "plot_prediction_scatter(y_test[regions], y_pred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
