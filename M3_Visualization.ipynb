{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d545b75d",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "### Import necessary modules and do some basic setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1427bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= '0.20'\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ >= '2.0'\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import dask\n",
    "import datetime\n",
    "import tabulate\n",
    "import math\n",
    "dask.config.set({'array.slicing.split_large_chunks': False})\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Config matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Custom utils\n",
    "from utils.utils_data import *\n",
    "from utils.utils_ml import *\n",
    "from utils.utils_plot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41766d4a",
   "metadata": {},
   "source": [
    "### Define some paths and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc1d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATADIR = os.path.join(os.getcwd(), '..', 'data')\n",
    "\n",
    "# Some constants\n",
    "DATE_START = '1979-01-01'\n",
    "DATE_END = '2020-12-31'\n",
    "YY_TRAIN = [1979, 2015]\n",
    "YY_TEST = [2016, 2020]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b2f95c",
   "metadata": {},
   "source": [
    "# Preparing precipitation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2436cd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read precipitation file\n",
    "df_prec = get_precipitation_data(DATADIR + '/MeteoSwiss/precip_regions.csv',\n",
    "                                 DATE_START, DATE_END)\n",
    "\n",
    "df_prec = prepare_prec_data_by_aggregated_regions(df_prec, qt=0.95)\n",
    "prec_cols = df_prec.columns[1:7]\n",
    "prec_xtr_cols = df_prec.columns[7:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a22e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select regions of interest for following analyses (for example only 'reg_tot' or all sub-regions)\n",
    "regions = ['reg_1', 'reg_2', 'reg_3', 'reg_4', 'reg_5', 'reg_tot']\n",
    "regions_xtr = ['reg_1_xtr', 'reg_2_xtr', 'reg_3_xtr', 'reg_4_xtr', 'reg_5_xtr', 'reg_tot_xtr']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c3dbae",
   "metadata": {},
   "source": [
    "# Preparing input data (predictors/features)\n",
    "\n",
    "### Load data and resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce5176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original gridded data\n",
    "z = get_era5_data(DATADIR + '/ERA5/geopotential/*.nc', DATE_START, DATE_END)\n",
    "z = z.sel(level=[500, 850, 1000])\n",
    "t2m = get_era5_data(DATADIR + '/ERA5/Daymean_era5_T2M_EU_19790101-20210905.nc', DATE_START, DATE_END)\n",
    "t2m['time'] = pd.DatetimeIndex(t2m.time.dt.date)\n",
    "\n",
    "lats = z.lat\n",
    "lons = z.lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe3ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to lower resolution\n",
    "resolution = 0.5\n",
    "\n",
    "if resolution > 0.25:\n",
    "    lons = np.arange(min(lons), max(lons), resolution)\n",
    "    lats = np.arange(min(lats), max(lats), resolution)\n",
    "    z = z.sel(lon=lons).sel(lat=lats)\n",
    "    t2m = t2m.sel(lon=lons).sel(lat=lats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3c4150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a level dimension to the 2D dataset\n",
    "t2m = t2m.expand_dims('level', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7643ef",
   "metadata": {},
   "source": [
    "### Split data and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a1b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split set into (training + validation) and testing based on dates\n",
    "z_train_full = z.sel(time=slice('{}-01-01'.format(YY_TRAIN[0]), '{}-12-31'.format(YY_TRAIN[1])))\n",
    "z_test = z.sel(time=slice('{}-01-01'.format(YY_TEST[0]), '{}-12-31'.format(YY_TEST[1])))\n",
    "t2m_train_full = t2m.sel(time=slice('{}-01-01'.format(YY_TRAIN[0]), '{}-12-31'.format(YY_TRAIN[1])))\n",
    "t2m_test = t2m.sel(time=slice('{}-01-01'.format(YY_TEST[0]), '{}-12-31'.format(YY_TEST[1])))\n",
    "\n",
    "y_train_full = df_prec[(df_prec.date.dt.year >= YY_TRAIN[0]) &\n",
    "                       (df_prec.date.dt.year <= YY_TRAIN[1])]\n",
    "y_test = df_prec[(df_prec.date.dt.year >= YY_TEST[0]) &\n",
    "                 (df_prec.date.dt.year <= YY_TEST[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dfaaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to numpy arrays and concatenate (takes time as it needs to load data from files)\n",
    "X_train_full = np.concatenate((np.squeeze(z_train_full.to_array().to_numpy(), axis=0),\n",
    "                               np.squeeze(t2m_train_full.to_array().to_numpy(), axis=0)), axis=1)\n",
    "X_test = np.concatenate((np.squeeze(z_test.to_array().to_numpy(), axis=0),\n",
    "                         np.squeeze(t2m_test.to_array().to_numpy(), axis=0)), axis=1)\n",
    "\n",
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcefb859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split full training into training and validation sets (and shuffle)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82652338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "X_mean = X_train.mean(axis=0, keepdims=True)\n",
    "X_std = X_train.std(axis=0, keepdims=True)\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "X_valid = (X_valid - X_mean) / X_std\n",
    "X_test = (X_test - X_mean) / X_std\n",
    "\n",
    "# Reshape data (set channel first; Con2D option data_format='channels_first' does not work on Win 10 64 bit)\n",
    "X_train = np.moveaxis(X_train, 1, -1)\n",
    "X_valid = np.moveaxis(X_valid, 1, -1)\n",
    "X_test = np.moveaxis(X_test, 1, -1)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e798e093",
   "metadata": {},
   "source": [
    "# CNN - Prediction of precipitation **extremes**\n",
    "\n",
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244465dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear session and set tf seed\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "INIT_LR = 1e-4\n",
    "MAX_LR = 1e-2\n",
    "\n",
    "# ANN using timeseries to predict precipitation\n",
    "cnn_xtrm_v1 = keras.models.Sequential([\n",
    "    layers.Input(shape=X_train.shape[1:]),\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(pool_size=2),\n",
    "    layers.SpatialDropout2D(0.2),\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(pool_size=2),\n",
    "    layers.SpatialDropout2D(0.2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(len(regions_xtr), activation='sigmoid')\n",
    "])\n",
    "\n",
    "cnn_xtrm_v1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d86cbb5",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca3c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "# Cyclical learning rate\n",
    "steps_per_epoch = len(X_train) // BATCH_SIZE\n",
    "clr = tfa.optimizers.CyclicalLearningRate(initial_learning_rate=INIT_LR,\n",
    "    maximal_learning_rate=MAX_LR,\n",
    "    scale_fn=lambda x: 1/(2.**(x-1)),\n",
    "    step_size=2 * steps_per_epoch\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(clr)\n",
    "\n",
    "# Compile model and train\n",
    "cnn_xtrm_v1.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=WeightedBinaryCrossEntropy(\n",
    "        pos_weight=5,\n",
    "        weight=1,\n",
    "        from_logits=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "history = cnn_xtrm_v1.fit(X_train,\n",
    "        y_train[regions_xtr].astype(float),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_valid, y_valid[regions_xtr].astype(float)),\n",
    "        epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e31f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training evolution\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49ee9c0",
   "metadata": {},
   "source": [
    "### Predict the test period and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe4c64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate the extremes\n",
    "y_pred_train = cnn_xtrm_v1.predict(X_train)\n",
    "y_pred_test = cnn_xtrm_v1.predict(X_test)\n",
    "\n",
    "y_pred_train_bool = y_pred_train >= 0.5\n",
    "y_pred_test_bool = y_pred_test >= 0.5\n",
    "\n",
    "# Confusion matrix per region (x: prediction; y: true value)\n",
    "for idx, region in enumerate(regions_xtr):\n",
    "    cnf_matrix = confusion_matrix(y_test[region], y_pred_test_bool[:, idx])\n",
    "    print(f\"Confusion matrix {region}:\\n {cnf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b199fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for the whole country\n",
    "evaluate_model(y_test.reg_tot_xtr, y_train.reg_tot_xtr, y_pred_test_bool[:, -1],\n",
    "                y_pred_test[:, -1], y_pred_train_bool[:, -1], y_pred_train[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd059eef",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "## Visualizing filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abd7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_weights = False\n",
    "\n",
    "# Extract weights from convolutional layers\n",
    "for layer in cnn_xtrm_v1.layers:\n",
    "    if 'conv' in layer.name:\n",
    "        weights, bias = layer.get_weights()\n",
    "        print(f'Filters of layer {layer.name} with shape {weights.shape}:')\n",
    "\n",
    "        # Optionally normalize filter values for visualization\n",
    "        if normalize_weights:\n",
    "            filters = (weights - weights.min()) / (weights.max() - weights.min())\n",
    "        else:\n",
    "            filters = weights\n",
    "\n",
    "        # Create figure\n",
    "        fig = plt.figure(figsize=[8, 15])\n",
    "\n",
    "        # Plotting the filters\n",
    "        filter_cnt = 1\n",
    "        for i_unit in range(filters.shape[3]):\n",
    "\n",
    "            # Get the filters for the selected unit\n",
    "            filter = filters[:, :, :, i_unit]\n",
    "\n",
    "            # Plotting each of the channel\n",
    "            for i_channel in range(filters.shape[2]):\n",
    "                ax = plt.subplot(filters.shape[3], filters.shape[2], filter_cnt)\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                plt.imshow(filter[:, :, i_channel])\n",
    "                filter_cnt += 1\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dae2359",
   "metadata": {},
   "source": [
    "## Visualizing feature maps\n",
    "\n",
    "### Extract CNN layers and create the visualization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1e9148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract layer names and outputs\n",
    "layer_names = [layer.name for layer in cnn_xtrm_v1.layers]\n",
    "layer_outputs = [layer.output for layer in cnn_xtrm_v1.layers]\n",
    "layer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c899e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the visualization model\n",
    "feature_map_model = tf.keras.models.Model(inputs=cnn_xtrm_v1.input,\n",
    "                                          outputs=layer_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e25043",
   "metadata": {},
   "source": [
    "### Create plotting and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c5f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions to plot predictors\n",
    "\n",
    "def add_to_plot(fig, axs, var, title, i, j, lons, lats):\n",
    "    im = axs[i, j].pcolormesh(lons, lats, var[0,:,:], cmap=mpl.cm.RdYlBu_r, shading='auto')\n",
    "\n",
    "    world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "    world.boundary.plot(ax=axs[i, j], color='k')\n",
    "\n",
    "    axs[i, j].set_xlim(min(lons), max(lons))\n",
    "    axs[i, j].set_ylim(min(lats), max(lats))\n",
    "    axs[i, j].set_title(title)\n",
    "\n",
    "    fig.colorbar(im, ax=axs[i, j])\n",
    "\n",
    "def plot_predictors(z, t2m, date_slct):\n",
    "\n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(2, 2, figsize=[12, 7])\n",
    "\n",
    "    # Add plots\n",
    "    z_xtreme = z.sel(time=date_slct)\n",
    "    t2m_xtreme = t2m.sel(time=date_slct)\n",
    "\n",
    "    add_to_plot(fig, axs, z_xtreme.sel(level=500).to_array(), 'Z500', 0, 0, lons, lats)\n",
    "    add_to_plot(fig, axs, z_xtreme.sel(level=850).to_array(), 'Z850', 0, 1, lons, lats)\n",
    "    add_to_plot(fig, axs, z_xtreme.sel(level=1000).to_array(), 'Z1000', 1, 0, lons, lats)\n",
    "    add_to_plot(fig, axs, t2m_xtreme.sel(level=0).to_array(), 'T2m', 1, 1, lons, lats)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b23c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to plot feature maps\n",
    "\n",
    "def plot_feature_maps(feature_maps):\n",
    "    scale_features = True\n",
    "    fig_ratio = 1\n",
    "\n",
    "    for layer_name, feature_map in zip(layer_names, feature_maps):\n",
    "        # Get rid of dense layers\n",
    "        if len(feature_map.shape) < 4:\n",
    "            continue\n",
    "\n",
    "        # Get rid of normalization and droupout layers\n",
    "        if 'normalization' in layer_name or 'dropout' in layer_name:\n",
    "            continue\n",
    "\n",
    "        print(f\"The shape of {layer_name} is {feature_map.shape}\")\n",
    "\n",
    "        # Extract all features and their sizes\n",
    "        n_features = feature_map.shape[-1]\n",
    "        size_h = feature_map.shape[1]\n",
    "        size_v = feature_map.shape[2]\n",
    "        fig_ratio = size_v / size_h\n",
    "\n",
    "        # Create the figure\n",
    "        n_cols = 4\n",
    "        n_rows = math.ceil(n_features / n_cols)\n",
    "        scale = 10. / n_cols\n",
    "        fig, axs = plt.subplots(n_rows, n_cols,\n",
    "                                figsize=(scale * n_cols * fig_ratio, scale * n_rows))\n",
    "\n",
    "        # Load country outlines\n",
    "        world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "        # Process the lat/lon axes\n",
    "        ax_lons = np.linspace(min(lons), max(lons), size_v)\n",
    "        ax_lats = np.linspace(min(lats), max(lats), size_h)\n",
    "\n",
    "        # Extract all features\n",
    "        for i_feature in range(n_features):\n",
    "            x = feature_map[0, :, :, i_feature]\n",
    "\n",
    "            # Scale to improve visibility\n",
    "            if scale_features:\n",
    "                x -= x.mean()\n",
    "                x /= x.std()\n",
    "                x *= 64\n",
    "                x += 128\n",
    "                x = np.clip(x, 0, 255).astype('uint8')\n",
    "\n",
    "            # Plot feature\n",
    "            i_row = i_feature % n_cols\n",
    "            i_col = i_feature // n_cols\n",
    "            im = axs[i_row, i_col].pcolormesh(ax_lons, ax_lats, x, cmap='viridis', shading='auto')\n",
    "\n",
    "            # Add country contours\n",
    "            world.boundary.plot(ax=axs[i_row, i_col], color='k')\n",
    "            axs[i_row, i_col].set_xlim(min(lons), max(lons))\n",
    "            axs[i_row, i_col].set_ylim(min(lats), max(lats))\n",
    "            axs[i_row, i_col].axis('off')\n",
    "\n",
    "        fig.suptitle(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7d79cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to select the best predicted date\n",
    "\n",
    "def get_best_predicted_date(target, y_pred_test, y_test, X_test, regions_xtr):\n",
    "    target_col = regions_xtr.index(target)\n",
    "    pred_prob = y_pred_test[y_test[target] == True, target_col]\n",
    "    i_xtreme = pred_prob.argmax()\n",
    "    date_xtreme = y_test[y_test[target] == True].iloc[i_xtreme].date\n",
    "    formatted_date = datetime.date.strftime(date_xtreme, \"%d.%m.%Y\")\n",
    "    print(f'An extreme event was predicted with a probability of {round(100 * pred_prob[i_xtreme], 1)}% for the {formatted_date}')\n",
    "\n",
    "    # Extract inputs for that date\n",
    "    data_xtreme = X_test[y_test[target] == True][i_xtreme]\n",
    "    data_xtreme = data_xtreme.reshape((1,) + data_xtreme.shape)\n",
    "\n",
    "    return date_xtreme, data_xtreme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b344967d",
   "metadata": {},
   "source": [
    "### Select days with extreme precipitation for different regions\n",
    "Regions: \n",
    "- reg_1_xtr (NW CH)\n",
    "- reg_2_xtr (N CH)\n",
    "- reg_3_xtr (Valais + Bernese Alps)\n",
    "- reg_4_xtr (E CH)\n",
    "- reg_5_xtr (Ticino)\n",
    "- reg_tot_xtr (total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e895ab",
   "metadata": {},
   "source": [
    "### Extreme precipitation events in the whole country\n",
    "\n",
    "Same date as for the Valais + Bernese Alps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0742150c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a day that was well predicted\n",
    "target = 'reg_tot_xtr'\n",
    "date_xtreme, data_xtreme = get_best_predicted_date(target, y_pred_test, y_test, X_test, regions_xtr)\n",
    "\n",
    "# Plot predictors\n",
    "plot_predictors(z, t2m, date_xtreme)\n",
    "\n",
    "# Feed the inputs of the extreme day into the model created\n",
    "feature_maps = feature_map_model.predict(data_xtreme)\n",
    "\n",
    "# Plot the feature maps\n",
    "plot_feature_maps(feature_maps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab21e67e",
   "metadata": {},
   "source": [
    "### Extreme precipitation events in NW Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdbc231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a day that was well predicted\n",
    "target = 'reg_1_xtr'\n",
    "date_xtreme, data_xtreme = get_best_predicted_date(target, y_pred_test, y_test, X_test, regions_xtr)\n",
    "\n",
    "# Plot predictors\n",
    "plot_predictors(z, t2m, date_xtreme)\n",
    "\n",
    "# Feed the inputs of the extreme day into the model created\n",
    "feature_maps = feature_map_model.predict(data_xtreme)\n",
    "\n",
    "# Plot the feature maps\n",
    "plot_feature_maps(feature_maps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682043e",
   "metadata": {},
   "source": [
    "### Extreme precipitation events in E Switzerland\n",
    "\n",
    "Same date as for N Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17817f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a day that was well predicted\n",
    "target = 'reg_4_xtr'\n",
    "date_xtreme, data_xtreme = get_best_predicted_date(target, y_pred_test, y_test, X_test, regions_xtr)\n",
    "\n",
    "# Plot predictors\n",
    "plot_predictors(z, t2m, date_xtreme)\n",
    "\n",
    "# Feed the inputs of the extreme day into the model created\n",
    "feature_maps = feature_map_model.predict(data_xtreme)\n",
    "\n",
    "# Plot the feature maps\n",
    "plot_feature_maps(feature_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a36e865",
   "metadata": {},
   "source": [
    "### Extreme precipitation events in Ticino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7f70e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a day that was well predicted\n",
    "target = 'reg_5_xtr'\n",
    "date_xtreme, data_xtreme = get_best_predicted_date(target, y_pred_test, y_test, X_test, regions_xtr)\n",
    "\n",
    "# Plot predictors\n",
    "plot_predictors(z, t2m, date_xtreme)\n",
    "\n",
    "# Feed the inputs of the extreme day into the model created\n",
    "feature_maps = feature_map_model.predict(data_xtreme)\n",
    "\n",
    "# Plot the feature maps\n",
    "plot_feature_maps(feature_maps)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
