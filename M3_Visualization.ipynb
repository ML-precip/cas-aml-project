{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Import necessary modules and do some basic setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= '0.20'\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ >= '2.0'\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Input, Dropout, MaxPooling2D, Flatten\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dask\n",
    "import math\n",
    "dask.config.set({'array.slicing.split_large_chunks': False})\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Config matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Custom utils\n",
    "from utils.utils_data import *\n",
    "from utils.utils_ml import *\n",
    "from utils.utils_plot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some paths and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATADIR = os.path.join(os.getcwd(), '..', 'data')\n",
    "\n",
    "# Some constants\n",
    "DATE_START = '1979-01-01'\n",
    "DATE_END = '2020-12-31'\n",
    "YY_TRAIN = [1979, 2015]\n",
    "YY_TEST = [2016, 2020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing precipitation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read precipitation file\n",
    "df_prec = get_precipitation_data(DATADIR + '/MeteoSwiss/precip_regions.csv',\n",
    "                                 DATE_START, DATE_END)\n",
    "\n",
    "df_prec = prepare_prec_data_by_aggregated_regions(df_prec, qt=0.95)\n",
    "prec_cols = df_prec.columns[1:7]\n",
    "prec_xtr_cols = df_prec.columns[7:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select regions of interest for following analyses (for example only 'reg_tot' or all sub-regions)\n",
    "regions = ['reg_1', 'reg_2', 'reg_3', 'reg_4', 'reg_5', 'reg_tot']\n",
    "regions_xtr = ['reg_1_xtr', 'reg_2_xtr', 'reg_3_xtr', 'reg_4_xtr', 'reg_5_xtr', 'reg_tot_xtr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Analysis 1: CNN - Using gridded data as input\n",
    "\n",
    "Objective: get some spatial information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing input data (predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid options (total extent: 80° lon & 50° lat)\n",
    "resolution = 1\n",
    "nb_lat = 20 * 1/resolution + 1\n",
    "nb_lon = 30 * 1/resolution + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gridded data\n",
    "ds_z = get_era5_data(DATADIR + '/ERA5/geopotential/*.nc', DATE_START, DATE_END)\n",
    "z = extract_points_around_CH(ds_z, step_lat=resolution, step_lon=resolution, nb_lat=nb_lat, nb_lon=nb_lon, levels=[300, 500, 700, 850, 1000])\n",
    "ds_t2m = get_era5_data(DATADIR + '/ERA5/Daymean_era5_T2M_EU_19790101-20210905.nc', DATE_START, DATE_END)\n",
    "t2m = extract_points_around_CH(ds_t2m, step_lat=resolution, step_lon=resolution, nb_lat=nb_lat, nb_lon=nb_lon)\n",
    "t2m['time'] = pd.DatetimeIndex(t2m.time.dt.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a level dimension to the 2D dataset\n",
    "t2m = t2m.expand_dims('level', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split set into (training + validation) and testing based on dates\n",
    "z_train_full = z.sel(time=slice('{}-01-01'.format(YY_TRAIN[0]), '{}-12-31'.format(YY_TRAIN[1])))\n",
    "z_test = z.sel(time=slice('{}-01-01'.format(YY_TEST[0]), '{}-12-31'.format(YY_TEST[1])))\n",
    "t2m_train_full = t2m.sel(time=slice('{}-01-01'.format(YY_TRAIN[0]), '{}-12-31'.format(YY_TRAIN[1])))\n",
    "t2m_test = t2m.sel(time=slice('{}-01-01'.format(YY_TEST[0]), '{}-12-31'.format(YY_TEST[1])))\n",
    "\n",
    "y_train_full = df_prec[(df_prec.date.dt.year >= YY_TRAIN[0]) &\n",
    "                       (df_prec.date.dt.year <= YY_TRAIN[1])]\n",
    "y_test = df_prec[(df_prec.date.dt.year >= YY_TEST[0]) &\n",
    "                 (df_prec.date.dt.year <= YY_TEST[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to numpy arrays and concatenate (takes time as it needs to load data from files)\n",
    "X_train_full = np.concatenate((np.squeeze(z_train_full.to_array().to_numpy(), axis=0),\n",
    "                               np.squeeze(t2m_train_full.to_array().to_numpy(), axis=0)), axis=1)\n",
    "X_test = np.concatenate((np.squeeze(z_test.to_array().to_numpy(), axis=0),\n",
    "                         np.squeeze(t2m_test.to_array().to_numpy(), axis=0)), axis=1)\n",
    "\n",
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split full training into training and validation sets (and shuffle)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "X_mean = X_train.mean(axis=0, keepdims=True)\n",
    "X_std = X_train.std(axis=0, keepdims=True)\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "X_valid = (X_valid - X_mean) / X_std\n",
    "X_test = (X_test - X_mean) / X_std\n",
    "\n",
    "# Reshape data (set channel first; Con2D option data_format='channels_first' does not work on Win 10 64 bit)\n",
    "X_train = np.moveaxis(X_train, 1, -1)\n",
    "X_valid = np.moveaxis(X_valid, 1, -1)\n",
    "X_test = np.moveaxis(X_test, 1, -1)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of precipitation **values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear session and set tf seed\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# CNN based on Davenport, F. V., & Diffenbaugh, N. S. (2021). Using Machine Learning \n",
    "# to Analyze Physical Causes of Climate Change: A Case Study of U.S. Midwest Extreme Precipitation. \n",
    "# Geophysical Research Letters, 48(15). https://doi.org/10.1029/2021GL093787\n",
    "cnn_prec_v1 = keras.models.Sequential([\n",
    "    Input(shape=X_train.shape[1:]),\n",
    "    Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(len(regions), activation='relu'),\n",
    "])\n",
    "\n",
    "cnn_prec_v1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model and train\n",
    "cnn_prec_v1.compile(loss='mse',\n",
    "                    optimizer='adam')\n",
    "\n",
    "history = cnn_prec_v1.fit(X_train, y_train.reg_tot, epochs=30,\n",
    "                          validation_data=(X_valid, y_valid.reg_tot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training evolution\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores per region\n",
    "y_pred = cnn_prec_v1.predict(X_test)\n",
    "scores = np.sqrt(np.square(np.subtract(y_test[regions], y_pred)).mean())\n",
    "scores.name = 'RMSE'\n",
    "print(scores.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of the predictions vs observations\n",
    "plot_prediction_scatter(y_test[regions], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of precipitation **extremes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear session and set tf seed\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ANN using timeseries to predict precipitation\n",
    "cnn_xtrm_v1 = keras.models.Sequential([\n",
    "    Input(shape=X_train.shape[1:]),\n",
    "    Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(len(regions_xtr), activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model and train\n",
    "cnn_xtrm_v1.compile(\n",
    "    optimizer='adam',\n",
    "    loss=WeightedBinaryCrossEntropy(\n",
    "        pos_weight=5,\n",
    "        weight=1,\n",
    "        from_logits=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "history = cnn_xtrm_v1.fit(X_train, y_train[regions_xtr].astype(float), epochs=30,\n",
    "                          validation_data=(X_valid, y_valid[regions_xtr].astype(float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate the extremes\n",
    "y_pred_train = cnn_xtrm_v1.predict(X_train)\n",
    "y_pred_test = cnn_xtrm_v1.predict(X_test)\n",
    "\n",
    "y_pred_train_bool = y_pred_train >= 0.5\n",
    "y_pred_test_bool = y_pred_test >= 0.5\n",
    "\n",
    "# Confusion matrix per region (x: prediction; y: true value)\n",
    "for idx, region in enumerate(regions_xtr):\n",
    "    cnf_matrix = confusion_matrix(y_test[region], y_pred_test_bool[:, idx])\n",
    "    print(f\"Confusion matrix {region}:\\n {cnf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for the whole country\n",
    "evaluate_model(y_test.reg_tot_xtr, y_train.reg_tot_xtr, y_pred_test_bool[:, -1],\n",
    "                y_pred_test[:, -1], y_pred_train_bool[:, -1], y_pred_train[:, -1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
