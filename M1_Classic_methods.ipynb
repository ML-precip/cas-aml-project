{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Import necessary modules and do some basic setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Custom utils for processing the data\n",
    "from utils_data import * \n",
    "from utils_ml import *\n",
    "\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Config matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Custom utils\n",
    "from utils_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some functions for plotting\n",
    "def plotprediction_TS(test_dates, final_predictions, test_labels):\n",
    "    import seaborn as sns\n",
    "    df_to_compare = pd.DataFrame({'date': test_dates, 'Actual': test_labels, 'Predicted': final_predictions})\n",
    "    dfm = pd.melt(df_to_compare, id_vars=['date'], value_vars=['Actual', 'Predicted'], var_name='data', value_name='precip')\n",
    "    f, axs = plt.subplots(1,2,\n",
    "                      figsize=(12,5),\n",
    "                      sharey=True)\n",
    "\n",
    "    sns.regplot(data= df_to_compare,\n",
    "                x=\"Actual\",\n",
    "                y=\"Predicted\",\n",
    "                ax=axs[0],\n",
    "                )\n",
    "    sns.lineplot(x='date', y='precip', hue = 'data', data=dfm, ax=axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(x, y):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(test_labels, preds)\n",
    "    ax.plot([test_labels.min(), test_labels.max()], [test_labels.min(), test_labels.max()], 'k--', lw=1)\n",
    "    ax.set_xlabel('Measured')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some paths and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATADIR = os.getcwd() + '/../data'\n",
    "\n",
    "# Some constants\n",
    "DATE_START = '1979-01-01'\n",
    "DATE_END = '2020-12-31'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning approaches\n",
    "\n",
    "## Getting started with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open data\n",
    "mslp = xr.open_mfdataset(DATADIR + '/ERA5/Day_era5_2deg_MSL_EU_1979-2021.nc', combine='by_coords')\n",
    "\n",
    "mslp = mslp.sel(time=slice(DATE_START, DATE_END))\n",
    "\n",
    "# Convert to hPa\n",
    "mslp.MSL.values = mslp.MSL.values/100\n",
    "lon = mslp.lon\n",
    "lat = mslp.lat\n",
    "dates = mslp.time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an example (day) of mslp\n",
    "mslp.MSL.isel(time=200).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process season mean\n",
    "seas_means = mslp.groupby(\"time.season\").mean()\n",
    "\n",
    "fg = seas_means.MSL.plot(col=\"season\",  col_wrap=4,\n",
    "    # The remaining kwargs customize the plot just as for not-faceted plots\n",
    "    robust=True,\n",
    "    cmap=mpl.cm.RdYlBu_r)\n",
    "\n",
    "# Use this to plot contours on each panel\n",
    "# Note that this plotting call uses the original DataArray gradients\n",
    "fg.map_dataarray(\n",
    "    xr.plot.contour, x=\"lon\", y=\"lat\", colors=\"k\", levels=13, add_colorbar=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute anomalies\n",
    "climatology = mslp.mean('time')\n",
    "\n",
    "# By season\n",
    "season_climatology = mslp.groupby('time.season').mean('time')\n",
    "\n",
    "# Climatological anomalies\n",
    "anom_mslp =  mslp.MSL  - climatology\n",
    "\n",
    "# By season\n",
    "anom_seas_mslp = mslp.groupby('time.season') - season_climatology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start using the whole data set for PCA. Then, anomalies can be used\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# We need to reshape the data [time,latxlon]  \n",
    "mslp_stacked = mslp.stack(latlon=('lat', 'lon'))\n",
    "\n",
    "# Load in memory for computing the PCA\n",
    "mslp_stacked.load()\n",
    "\n",
    "# Extract msl variable\n",
    "X = mslp_stacked.MSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of variables (features) is 1025 (41 points in longitude * 25 points in latitude)\n",
    "# Standardise the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler  = StandardScaler()\n",
    "scaler = scaler.fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the PCA\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how many components \n",
    "f, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(range(1,21), pca.explained_variance_ratio_[0:20]*100)\n",
    "ax.plot(range(1,21), pca.explained_variance_ratio_[0:20]*100,'ro')\n",
    "ax.grid(ls=':')\n",
    "ax.set_xticks(range(1,21)); \n",
    "ax.set_xlabel('PC#');\n",
    "ax.set_ylabel(\"% variance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can take 4 or 5\n",
    "# Following the literature I will take 4 (e.g. Cortesi et a., 2021)\n",
    "n = 12 # We can change this number\n",
    "pca.explained_variance_ratio_[:n].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCs = pca.fit_transform(X)\n",
    "PCs_n = PCs[:,:n]\n",
    "\n",
    "# Data frame format for the selected components\n",
    "PCdf = pd.DataFrame(PCs_n, index = mslp['time'], \\\n",
    "                    columns = [\"PC%s\" % (x) for x in range(1, PCs_n.shape[1] +1)])\n",
    "\n",
    "# See the data\n",
    "PCdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EOFS (Empirical orthogonal functions) contain the spatial patterns associated with each PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOFs = pca.components_\n",
    "EOFs = EOFs[:n,:]\n",
    "\n",
    "# Reshape the data\n",
    "EOFs_r = EOFs.reshape((n, len(lat), len(lon)))\n",
    "EOFs_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = []\n",
    "tot_var = []\n",
    "for ip in range(n):\n",
    "    xn = pca.explained_variance_ratio_[:ip + 1].sum()\n",
    "    nn.append(xn)\n",
    "    xx =  pca.explained_variance_ratio_[:ip + 1].sum() - pca.explained_variance_ratio_[:ip ].sum()\n",
    "    tot_var.append(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into Xarray for visualization\n",
    "XD_EOFs_r = xr.DataArray(data=EOFs_r, coords=[(\"PCA\", tot_var), (\"lat\", lat.data), (\"lon\", lon.data)])\n",
    "\n",
    "fg = XD_EOFs_r.plot(col=\"PCA\",  col_wrap=4,\n",
    "    # The remaining kwargs customize the plot just as for not-faceted plots\n",
    "    robust=True,\n",
    "    cmap=mpl.cm.RdYlBu_r)\n",
    "\n",
    "# Use this to plot contours on each panel\n",
    "# Note that this plotting call uses the original DataArray gradients\n",
    "fg.map_dataarray(\n",
    "    xr.plot.contour, x=\"lon\", y=\"lat\", colors=\"k\", levels=13, add_colorbar=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Perform K-cluster analysis using the PCds obtained before\n",
    "nclusters = 12\n",
    "kmeans = KMeans(init='k-means++', n_clusters=nclusters, n_init=10)\n",
    "kmeans.fit(PCdf.values)\n",
    "y_pred = kmeans.fit_predict(PCdf.values)\n",
    "\n",
    "# Each day belongs to a cluster, labelled by kmeands.labels_\n",
    "np.unique(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(kmeans.labels_, index=np.array(mslp['time']), columns=['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = pd.DataFrame(kmeans.labels_, index=mslp['time'], columns=['cluster'])\n",
    "\n",
    "# See how many days belong to cluster 0\n",
    "index = labels.query('cluster == {}'.format(0))\n",
    "len(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each cluster we calculate the mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tot = len(labels.cluster)\n",
    "clusters = []\n",
    "day_clusters = []\n",
    "nbdays = []\n",
    "\n",
    "for iclus in range(nclusters): \n",
    "    index = labels.query('cluster == {}'.format(iclus)) \n",
    "    freq = (len(index)/num_tot)*100\n",
    "    freq = round(freq,2)\n",
    "    nbdays.append(freq)\n",
    "    cluster = mslp.sel(time=index.index).mean('time')\n",
    "    Dcluster = mslp.sel(time=index.index)\n",
    "    clusters.append(cluster)\n",
    "    day_clusters.append(Dcluster)\n",
    "    \n",
    "clusters = xr.concat(clusters, dim='cluster')\n",
    "clusters.assign_coords(cluster=nbdays)\n",
    "\n",
    "#day_clusters = xr.concat(day_clusters, dim='cluster')\n",
    "#day_clusters.assign_coords(day_clusters=nbdays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_C = clusters.MSL.plot(col=\"cluster\",  col_wrap=4,\n",
    "    # The remaining kwargs customize the plot just as for not-faceted plots\n",
    "    robust=True, \n",
    "    cmap=mpl.cm.RdYlBu_r)\n",
    "\n",
    "fg_C.map_dataarray(\n",
    "    xr.plot.contour, x=\"lon\", y=\"lat\", colors=\"k\", levels=13, add_colorbar=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PCs and Clusters \n",
    "PCdf['date'] = PCdf.index\n",
    "PCdf.to_csv(DATADIR + 'PCdf.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ClusterData(day_clusters):\n",
    "    t_list = []\n",
    "    for iclus in range(0,len(day_clusters)):\n",
    "        print(iclus)\n",
    "        tmp = day_clusters[iclus].mean(dim=['lon', 'lat'])\n",
    "        tmp_df = pd.DataFrame({'date' : tmp['time'], 'MSL': tmp['MSL'], 'Cluster': iclus})\n",
    "        t_list.append(tmp_df)\n",
    "\n",
    "    # Merge by date \n",
    "    df = pd.concat(t_list)\n",
    "    df = df.sort_values(by=\"date\")\n",
    "    return(df)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cluster_spatialmean = prepare_ClusterData(day_clusters)\n",
    "Cluster_spatialmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cluster_spatialmean.to_csv(DATADIR + 'Cluster_spatialmean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation: precipitation time series\n",
    "\n",
    "**Dataset**: RhiresD, which is a gridded daily precipitation dataset over Switzerland provided by MeteoSwiss. It is based on a spatial interpolation of rain-gauge data. The grid resolution is 1 km, but the effective resolution is in the order of 15-20 km.\n",
    "\n",
    "\n",
    "**Aggregations levels**: The gridded dataset has been averaged over different regions:\n",
    "* 12 climatic regions\n",
    "* 5 aggregated regions\n",
    "* the whole country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip = get_precipitation_data(DATADIR + '/MeteoSwiss/precip_regions.csv', DATE_START, DATE_END)\n",
    "\n",
    "precip_p95 = precip_exceedance(precip, 0.95)\n",
    "precip_p99 = precip_exceedance(precip, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = read_csv_files(glob.glob(os.path.join(\n",
    "    DATADIR + '/ERA5/TS_CH/', '*.csv')), DATE_START, DATE_END)\n",
    "\n",
    "full_data = concat_dataframes([variables, precip.reg_tot])\n",
    "\n",
    "full_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data used: \n",
    "A set of primary meteorological variables are used as covariates.\n",
    "- Geopotential levels (1000,850,700,500,300)\n",
    "- MSL: Mean sea level pressure\n",
    "- T2MMEAN: 2m-temperature\n",
    "\n",
    "**Extra-variables such as lagged T2MMEAN and PCs will be used (see below)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset \n",
    "DIRCSV  = DATADIR + '/TS_CH/'\n",
    "l_files = glob.glob(os.path.join(DIRCSV, 'df*.csv'))\n",
    "df_vars =  read_csv_files(l_files, DATE_START, DATE_END)\n",
    "df_vars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lag-t2mmean: For precipiation the memory system is important, adding lagged variables might help to predict precipitation\n",
    "df_vars['T2MLag'] = df_vars['T2MMEAN'].shift(1)\n",
    "df_vars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATADIR### Use PCs from the previous steps (PCA-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Large scale-atmospheric PCs and Clusters\n",
    "df_PCs = pd.read_csv(DATADIR + '/ERA5/PCdf.csv')\n",
    "df_PCs['date'] = pd.DatetimeIndex(df_PCs['date']).normalize()\n",
    "df_clusters =  pd.read_csv(DATADIR + '/ERA5/Cluster_spatialmean.csv')\n",
    "df_clus    = df_clusters[['date','Cluster']]\n",
    "df_clus['date'] = pd.DatetimeIndex(df_clus['date']).normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TARGET: read precipitaion\n",
    "df_prec = get_precipitation_data ( DATADIR + '/TS_CH/precip_regions.csv', DATE_START, DATE_END)\n",
    "# Select the right columns: date and reg_tot (all country)\n",
    "df_prec = df_prec[['date','reg_tot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge everything\n",
    "l_all = []\n",
    "l_all.append(df_vars)\n",
    "l_all.append(df_prec)\n",
    "df_all = concat_dataframes(l_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data exploration\n",
    "import seaborn\n",
    "seaborn.pairplot(df_all, vars=df_all.columns[1:9],\n",
    "                 kind='reg')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy_train = [1979,2015]\n",
    "yy_test  = [2016,2020]\n",
    "ylabel = df_prec.columns[1]\n",
    "\n",
    "# Add categorical variables\n",
    "#df_input = pd.merge(df_all, df_clus)\n",
    "df_input = pd.merge(df_all, df_PCs)\n",
    "names_col = df_input.columns\n",
    "# define attributes - i.e covariates: I will also remove MSL as I am using the PCs\n",
    "attributes = names_col.drop(['date','reg_tot','MSL'])\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the target variable: precipitation\n",
    "# Plot Time series\n",
    "from matplotlib import pyplot\n",
    "df_input['reg_tot'].plot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_labels, test_dataset, test_labels, train_dates, test_dates = split_data(df_input, yy_train, yy_test, attributes, ylabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_var='Cluster' # in case of using the clusters, it has to be a categorical variable.\n",
    "# In the following we won't use categorical variables, as we're using the PCs\n",
    "fpipeline = prepareData(train_dataset, None)\n",
    "X_prep_train = fpipeline.fit_transform(train_dataset)\n",
    "X_prep_test = fpipeline.fit_transform(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prep_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multiple Linear regression for precipitation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libraries\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(n_jobs=16)\n",
    "lr.fit(X_prep_train, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train = mean_squared_error(train_labels, lr.predict(X_prep_train))\n",
    "mse_test = mean_squared_error(test_labels, lr.predict(X_prep_test))\n",
    "print(f'Train MSE = {mse_train}'); print(f'Test MSE = {mse_test}')\n",
    "print(f'Train RMSE = {np.sqrt(mse_train)}'); print(f'Test RMSE = {np.sqrt(mse_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(lr)             \n",
    "rfe = rfe.fit(X_prep_train, train_labels)\n",
    "mean_squared_error(train_labels, rfe.predict(X_prep_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_val_predict returns an array of the same size as `y` where each entry\n",
    "# is a prediction obtained by cross validated:\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "lr_cv_mse = cross_val_score(lr, X_prep_train, train_labels, scoring='neg_mean_squared_error', cv=10)\n",
    "# We got the negative average MSE for cross-validation (minimizing MSE is equivalent to maximizing the negative MSE)\n",
    "lr_cv_mse.mean()\n",
    "# The result is close to what we obtained before. The negative result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_rmse_scores = np.sqrt(-lr_cv_mse)\n",
    "pd.Series(lin_rmse_scores).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Folds: \" + str(len(lr_cv_mse)) + \", MSE: \" + str(np.mean(np.abs(lr_cv_mse))) + \", STD: \" + str(np.std(lr_cv_mse)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the coefficients\n",
    "lr.coef_\n",
    "#coeff_df = pd.DataFrame(lr.coef_, attributes, columns=['Coefficient'])\n",
    "# makes some predictions\n",
    "y_pred = lr.predict(X_prep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotprediction_TS(test_dates, y_pred, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we're getting negative predicted values, let's try to transform the target variable \n",
    "# Transform targets and use same linear model\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "regr_trans = TransformedTargetRegressor(regressor=LinearRegression(),\n",
    "                                        func=np.log1p,\n",
    "                                        inverse_func=np.expm1)\n",
    "\n",
    "\n",
    "regr_trans.fit(X_prep_train, train_labels)\n",
    "y_trans_pred = regr_trans.predict(X_prep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_transf_train = mean_squared_error(train_labels, regr_trans.predict(X_prep_train))\n",
    "mse_transf_test = mean_squared_error(test_labels, regr_trans.predict(X_prep_test))\n",
    "print(f'Train MSE = {mse_transf_train}'); print(f'Test MSE = {mse_transf_test}')\n",
    "print(f'Train RMSE = {np.sqrt(mse_transf_train)}'); print(f'Test RMSE = {np.sqrt(mse_transf_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot predictions\n",
    "plotprediction_TS(test_dates, y_trans_pred, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The predictions seem to be better, but the MSE are still large*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "forest_reg.fit(X_prep_train, train_labels)\n",
    "# make predictions\n",
    "y_rf_pred = forest_reg.predict(X_prep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_rf_train = mean_squared_error(train_labels, forest_reg.predict(X_prep_train))\n",
    "mse_rf_test = mean_squared_error(test_labels, forest_reg.predict(X_prep_test))\n",
    "print(f'Train MSE = {mse_rf_train}'); print(f'Test MSE = {mse_rf_test}')\n",
    "print(f'Train RMSE = {np.sqrt(mse_rf_train)}'); print(f'Test RMSE = {np.sqrt(mse_rf_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see overfitting problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tunning parameter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    # try 12 (3×4) combinations of hyperparameters\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6]},\n",
    "    # then try 6 (2×3) combinations with bootstrap set as False\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "# Create the parameter grid based on the results of random search \n",
    "    \n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(X_prep_train, train_labels)\n",
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Best Score:\" , grid_search.best_score_)\n",
    "print (\"Best params:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_GCV_reg = RandomForestRegressor(n_jobs=-1).set_params(**best_params)\n",
    "forest_GCV_reg.fit(X_prep_train,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rf_cv_predict = forest_GCV_reg.predict(X_prep_test)\n",
    "mse_rf_cv_train = mean_squared_error(train_labels, forest_GCV_reg.predict(X_prep_train))\n",
    "mse_rf_cv_test = mean_squared_error(test_labels, forest_GCV_reg.predict(X_prep_test))\n",
    "print(f'Train MSE = {mse_rf_cv_train}'); print(f'Test MSE = {mse_rf_cv_test}')\n",
    "print(f'Train RMSE = {np.sqrt(mse_rf_cv_train)}'); print(f'Test RMSE = {np.sqrt(mse_rf_cv_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotprediction_TS(test_dates, y_rf_cv_predict, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importance = forest_GCV_reg.feature_importances_\n",
    "sorted_features_importance = sorted(zip(features_importance, attributes), reverse=True)\n",
    "sorted_features_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(features_importance)\n",
    "plt.barh(range(len(attributes)), features_importance[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [attributes[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting extremes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define exceedances based on the 95th\n",
    "df_prec_ex = precip_exceedance(df_prec)\n",
    "df_prec_ex['reg_tot'] = df_prec_ex['reg_tot']*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_ex = df_input\n",
    "# Replace reg_tot by the exceedances\n",
    "df_input_ex['reg_tot'] = df_prec_ex['reg_tot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_labels, test_dataset, test_labels, train_dates, test_dates = split_data(df_input_ex, yy_train, yy_test, attributes, ylabel)\n",
    "# but the data is already in the format (only the labels have been replace by the exceedances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# all parameters not specified are set to their defaults\n",
    "logisticRegr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "logisticRegr.fit(X_prep_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ex_pred=logisticRegr.predict(X_prep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the metrics class\n",
    "from sklearn import metrics\n",
    "cnf_matrix = metrics.confusion_matrix(test_labels, y_ex_pred)\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 7 false positive, and 70  (70+7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use score method to get accuracy of model\n",
    "score = logisticRegr.score(X_prep_test, test_labels)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, y_ex_pred, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve\n",
    "\n",
    "# Make probability predictions\n",
    "train_probs = logisticRegr.predict_proba(X_prep_train)[:, 1]\n",
    "probs = logisticRegr.predict_proba(X_prep_test)[:, 1]\n",
    "\n",
    "train_predictions = logisticRegr.predict(X_prep_train)\n",
    "predictions = logisticRegr.predict(X_prep_test)\n",
    "print(f'Train ROC AUC Score: {roc_auc_score(train_labels, train_probs)}')\n",
    "print(f'Test ROC AUC  Score: {roc_auc_score(test_labels, probs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(test_labels,train_labels, predictions, probs, train_predictions, train_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the model with 100 trees\n",
    "rf_class = RandomForestClassifier(n_estimators=150, \n",
    "                               random_state=42, \n",
    "                               max_features = 'sqrt',\n",
    "                               n_jobs=-1, verbose = 1)\n",
    "\n",
    "# Fit on training data\n",
    "rf_class.fit(X_prep_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = []\n",
    "max_depths = []\n",
    "\n",
    "for ind_tree in rf_class.estimators_:\n",
    "    n_nodes.append(ind_tree.tree_.node_count)\n",
    "    max_depths.append(ind_tree.tree_.max_depth)\n",
    "    \n",
    "print(f'Average number of nodes {int(np.mean(n_nodes))}')\n",
    "print(f'Average maximum depth {int(np.mean(max_depths))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rf_predictions = rf_class.predict(X_prep_train)\n",
    "train_rf_probs = rf_class.predict_proba(X_prep_train)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions = rf_class.predict(X_prep_test)\n",
    "rf_probs = rf_class.predict_proba(X_prep_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(test_labels, train_labels, rf_predictions, rf_probs,train_rf_predictions, train_rf_probs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
