{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a82c99b",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Import necessary modules and define some paths and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f2608ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Config matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2b4a6c1-5809-4e41-869d-62964ea76f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATADIR = os.getcwd() + '/../data'\n",
    "\n",
    "# Some constants\n",
    "G = 9.80665\n",
    "CH_CENTER = [46.818, 8.228]\n",
    "CH_BOUNDING_BOX = [45.66, 47.87, 5.84, 10.98]\n",
    "DATE_START = '1979-01-01'\n",
    "DATE_END = '2019-12-31'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5bb32e",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "## Target variable: precipitation time series\n",
    "\n",
    "**Dataset**: RhiresD, which is a gridded daily precipitation dataset over Switzerland provided by MeteoSwiss. It is based on a spatial interpolation of rain-gauge data. The grid resolution is 1 km, but the effective resolution is in the order of 15-20 km.\n",
    "\n",
    "\n",
    "**Aggregations levels**: The gridded dataset has been averaged over different regions:\n",
    "* 12 climatic regions\n",
    "* 5 aggregated regions\n",
    "* the whole country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4677a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read precipitation file and get events over threshold\n",
    "precip = pd.read_csv(DATADIR + '/MeteoSwiss/precip_regions.csv')\n",
    "\n",
    "df_time = pd.to_datetime({\n",
    "    'year': precip.year,\n",
    "    'month': precip.month,\n",
    "    'day': precip.day})\n",
    "precip.insert(0, \"date\", df_time, True)\n",
    "\n",
    "precip = precip[(precip.date >= DATE_START) & (precip.date <= DATE_END)]\n",
    "\n",
    "precip_p95 = precip.copy()\n",
    "precip_p99 = precip.copy()\n",
    "\n",
    "for key, ts in precip.iteritems():\n",
    "    if key in ['year', 'month', 'day']: continue\n",
    "    precip_p95[key] = ts > ts.quantile(0.95)\n",
    "    precip_p99[key] = ts > ts.quantile(0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0de1f4",
   "metadata": {},
   "source": [
    "## Predictors: meteorological fields\n",
    "\n",
    "**Dataset**: ERA5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5ba1614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data extraction functions for ERA5\n",
    "\n",
    "def rename_dimensions_variables(ds):\n",
    "    \"\"\"Rename dimensions and attributes of the given dataset to homogenize data.\"\"\"\n",
    "    if 'latitude' in ds.dims:\n",
    "        ds = ds.rename({'latitude': 'lat'})\n",
    "    if 'longitude' in ds.dims:\n",
    "        ds = ds.rename({'longitude': 'lon'})\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def get_era5_data(files, start=DATE_START, end=DATE_END):\n",
    "    \"\"\"Extract ERA5 data for the given file(s) pattern/path.\"\"\"\n",
    "    print('Extracting data for the period {} - {}'.format(start, end))\n",
    "    ds = xr.open_mfdataset(DATADIR + '/ERA5/' + files, combine='by_coords')\n",
    "    ds = rename_dimensions_variables(ds)\n",
    "    ds = ds.sel(\n",
    "        time=slice(start, end)\n",
    "    )\n",
    "\n",
    "    return ds\n",
    "\n",
    "    \n",
    "def extract_nearest_point_data(ds, lat, lon):\n",
    "    \"\"\"Return the time series data for the nearest grid point.\n",
    "\n",
    "    Arguments:\n",
    "        ds -- the dataset (xarray Dataset) to extract the data from\n",
    "        lat -- the latitude coordinate of the point of interest\n",
    "        lon -- the longitude coordinate of the point of interest\n",
    "\n",
    "    Example:\n",
    "    z = xr.open_mfdataset(DATADIR + '/ERA5/geopotential/*.nc', combine='by_coords')\n",
    "    a = extract_nearest_point_data(z, CH_CENTER[0], CH_CENTER[1])\n",
    "    \"\"\"\n",
    "    return ds.sel({'lat': lat, 'lon': lon}, method=\"nearest\")\n",
    "\n",
    "\n",
    "def extract_points_around(ds, lat, lon, step_lat, step_lon, nb_lat, nb_lon):\n",
    "    \"\"\"Return the time series data for a grid point mesh around the provided coordinates.\n",
    "    \n",
    "    Arguments:\n",
    "    ds -- the dataset (xarray Dataset) to extract the data from\n",
    "    lat -- the latitude coordinate of the center of the mesh\n",
    "    lon -- the longitude coordinate of the center of the mesh\n",
    "    step_lat -- the step in latitude of the mesh\n",
    "    step_lon -- the step in longitude of the mesh\n",
    "    nb_lat -- the total number of grid points to extract for the latitude axis (the mesh will be centered)\n",
    "    nb_lon -- the total number of grid points to extract for the longitude axis (the mesh will be centered)\n",
    "\n",
    "    Example:\n",
    "    z = xr.open_mfdataset(DATADIR + '/ERA5/geopotential/*.nc', combine='by_coords')\n",
    "    a = extract_points_around(z, CH_CENTER[0], CH_CENTER[1], step_lat=1, step_lon=1, nb_lat=3, nb_lon=3)\n",
    "    \"\"\"\n",
    "    lats = np.arange(lat - step_lat * (nb_lat - 1) / 2, lat + step_lat * nb_lat / 2, step_lat)\n",
    "    lons = np.arange(lon - step_lon * (nb_lon - 1) / 2, lon + step_lon * nb_lon / 2, step_lon)\n",
    "    xx, yy = np.meshgrid(lats, lons)\n",
    "    xx = xx.flatten()\n",
    "    yy = yy.flatten()\n",
    "    xys = np.column_stack((xx, yy))\n",
    "    \n",
    "    data = []\n",
    "    for xy in xys:\n",
    "        data.append(extract_nearest_point_data(ds, xy[0], xy[1]))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_data_mean_over_box(ds, lats, lons, level = 0):\n",
    "    \"\"\"Extract data from points within a bounding box and process the mean.\n",
    "    \n",
    "    Arguments:\n",
    "    ds -- the dataset (xarray Dataset) to extract the data from\n",
    "    lats -- the min/max latitude coordinates of the bounding box\n",
    "    lons -- the min/max longitude coordinates of the bounding box\n",
    "    level -- the desired vertical level\n",
    "    \"\"\"\n",
    "    if len(lats) != 2:\n",
    "        raise Exception('An array of length 2 is expected for the lats.')\n",
    "    if len(lons) != 2:\n",
    "        raise Exception('An array of length 2 is expected for the lons.')\n",
    "\n",
    "    lat_start = min(lats)\n",
    "    lat_end = max(lats)\n",
    "\n",
    "    if (ds.lat[0] > ds.lat[1]):\n",
    "        lat_start = max(lats)\n",
    "        lat_end = min(lats)\n",
    "\n",
    "    if 'level' in ds.dims:\n",
    "        ds_box = ds.sel(\n",
    "            lat=slice(lat_start, lat_end), lon=slice(min(lons), max(lons)), level=level\n",
    "        )\n",
    "    else:\n",
    "        ds_box = ds.sel(\n",
    "            lat=slice(lat_start, lat_end), lon=slice(min(lons), max(lons))\n",
    "        )\n",
    "\n",
    "    return ds_box.mean(['lat', 'lon'])\n",
    "\n",
    "\n",
    "def get_data_mean_over_CH_box(ds, level = 0):\n",
    "    \"\"\"Extract data over the bounding box of Switzerland and return the mean time series.\n",
    "    \n",
    "    Arguments:\n",
    "    level -- the desired vertical level\n",
    "    \"\"\"\n",
    "    return get_data_mean_over_box(ds, [CH_BOUNDING_BOX[0], CH_BOUNDING_BOX[1]], [CH_BOUNDING_BOX[2], CH_BOUNDING_BOX[3]], level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78f897",
   "metadata": {},
   "source": [
    "# Unsupervised learning approaches\n",
    "\n",
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621e452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "827f12bd",
   "metadata": {},
   "source": [
    "## K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2806047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98377b0a",
   "metadata": {},
   "source": [
    "# Supervised learning approaches\n",
    "\n",
    "## Linear regression for precipitation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "542e8eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data for the period 1979-01-01 - 2019-12-31\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "found the following matches with the input file in xarray's IO backends: ['netcdf4', 'h5netcdf']. But their dependencies may not be installed, see:\nhttp://xarray.pydata.org/en/stable/user-guide/io.html \nhttp://xarray.pydata.org/en/stable/getting-started-guide/installing.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-327bbf564ea6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Open data and get the mean value over Switzerland\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_era5_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'geopotential/*.nc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mz500_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_mean_over_CH_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmslp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_era5_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mslp/*.nc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmslp_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_mean_over_CH_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmslp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-4d852a4d1c43>\u001b[0m in \u001b[0;36mget_era5_data\u001b[0;34m(files, start, end)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m\"\"\"Extract ERA5 data for the given file(s) pattern/path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Extracting data for the period {} - {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_mfdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATADIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/ERA5/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'by_coords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrename_dimensions_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     ds = ds.sel(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[0m\n\u001b[1;32m    911\u001b[0m         \u001b[0mgetattr_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m     \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mopen_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m     \u001b[0mclosers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_close\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    911\u001b[0m         \u001b[0mgetattr_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m     \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mopen_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m     \u001b[0mclosers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_close\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, backend_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguess_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/xarray/backends/plugins.py\u001b[0m in \u001b[0;36mguess_engine\u001b[0;34m(store_spec)\u001b[0m\n\u001b[1;32m    146\u001b[0m         )\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: found the following matches with the input file in xarray's IO backends: ['netcdf4', 'h5netcdf']. But their dependencies may not be installed, see:\nhttp://xarray.pydata.org/en/stable/user-guide/io.html \nhttp://xarray.pydata.org/en/stable/getting-started-guide/installing.html"
     ]
    }
   ],
   "source": [
    "# Open data and get the mean value over Switzerland\n",
    "z = get_era5_data('geopotential/*.nc')\n",
    "z500_mean = get_data_mean_over_CH_box(z, 500)\n",
    "mslp = get_era5_data('mslp/*.nc')\n",
    "mslp_mean = get_data_mean_over_CH_box(mslp)\n",
    "t2m = get_era5_data('Daymean_era5_T2M_EU_19790101-20210905.nc')\n",
    "t2m_mean = get_data_mean_over_CH_box(t2m)\n",
    "\n",
    "# Convert to geopotential height and hPa\n",
    "z500_mean['z'] = z500_mean['z'] / G\n",
    "mslp_mean['MSL'] = mslp_mean['MSL'] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4355498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(20,5))\n",
    "z500_mean.z.plot(ax=axs[0])\n",
    "axs[0].set_title('Geopotentiel 500hPa')\n",
    "mslp_mean.MSL.plot(ax=axs[1])\n",
    "axs[1].set_title('Sea level pressure')\n",
    "t2m_mean.T2MMEAN.plot(ax=axs[2])\n",
    "axs[2].set_title('Temperature 2m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d483f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "full_set = [z500_mean, mslp_mean, t2m_mean, precip]\n",
    "\n",
    "#train_set, test_set = train_test_split(full_set, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5827d889",
   "metadata": {},
   "source": [
    "## Logistic regression for extreme events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69c3005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e6ae995",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dcb629",
   "metadata": {},
   "source": [
    "# Deep learning approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98826a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
