{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "\n",
    "Import necessary modules and do some basic setup."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Config matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Custom utils\n",
    "from utils import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define some paths and constants."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Paths\n",
    "DATADIR = os.getcwd() + '/../data'\n",
    "\n",
    "# Some constants\n",
    "G = 9.80665\n",
    "CH_CENTER = [46.818, 8.228]\n",
    "CH_BOUNDING_BOX = [45.66, 47.87, 5.84, 10.98]\n",
    "DATE_START = '1979-01-01'\n",
    "DATE_END = '2020-12-31'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Unsupervised learning approaches\n",
    "\n",
    "## Getting started with the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Open data\n",
    "mslp = xr.open_mfdataset(DATADIR + '/ERA5/Daymean_era5_2deg_MSL_EU_19790101-20210902.nc', combine='by_coords')\n",
    "mslp = mslp.sel(time=slice(DATE_START, DATE_END))\n",
    "\n",
    "# Convert to hPa\n",
    "mslp.MSL.values = mslp.MSL.values/100\n",
    "lon = mslp.lon\n",
    "lat = mslp.lat\n",
    "\n",
    "mslp.MSL.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot an example (day) of mslp\n",
    "mslp.MSL.isel(time=200).plot();"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Process season mean\n",
    "seas_means = mslp.groupby(\"time.season\").mean()\n",
    "\n",
    "fg = seas_means.MSL.plot(col=\"season\",  col_wrap=4,\n",
    "    # The remaining kwargs customize the plot just as for not-faceted plots\n",
    "    robust=True,\n",
    "    cmap=mpl.cm.RdYlBu_r)\n",
    "\n",
    "# Use this to plot contours on each panel\n",
    "# Note that this plotting call uses the original DataArray gradients\n",
    "fg.map_dataarray(\n",
    "    xr.plot.contour, x=\"lon\", y=\"lat\", colors=\"k\", levels=13, add_colorbar=False\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Compute anomalies\n",
    "climatology = mslp.mean('time')\n",
    "\n",
    "# By season\n",
    "season_climatology = mslp.groupby('time.season').mean('time')\n",
    "\n",
    "# Climatological anomalies\n",
    "anom_mslp =  mslp.MSL  - climatology\n",
    "\n",
    "# By season\n",
    "anom_seas_mslp = mslp.groupby('time.season') - season_climatology"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PCA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Start using the whole data set for PCA. Then, anomalies can be used\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# We need to reshape the data [time,latxlon]  \n",
    "mslp_stacked = mslp.stack(latlon=('lat', 'lon'))\n",
    "\n",
    "# Load in memory for computing the PCA\n",
    "mslp_stacked.load()\n",
    "\n",
    "# Extract msl variable\n",
    "X = mslp_stacked.MSL"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The number of variables (features) is 1025 (41 points in longitude * 25 points in latitude)\n",
    "# Standardise the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler  = StandardScaler()\n",
    "scaler = scaler.fit(X)\n",
    "X = scaler.transform(X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Do the PCA\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# See how many components \n",
    "f, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(range(1,21), pca.explained_variance_ratio_[0:20]*100)\n",
    "ax.plot(range(1,21), pca.explained_variance_ratio_[0:20]*100,'ro')\n",
    "ax.grid(ls=':')\n",
    "ax.set_xticks(range(1,21)); \n",
    "ax.set_xlabel('PC#');\n",
    "ax.set_ylabel(\"% variance\");"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We can take 4 or 5\n",
    "# Following the literature I will take 4 (e.g. Cortesi et a., 2021)\n",
    "n = 12 # We can change this number\n",
    "pca.explained_variance_ratio_[:n].sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "PCs = pca.fit_transform(X)\n",
    "PCs_n = PCs[:,:n]\n",
    "\n",
    "# Data frame format for the selected components\n",
    "PCdf = pd.DataFrame(PCs_n, index = mslp['time'], \\\n",
    "                    columns = [\"PC%s\" % (x) for x in range(1, PCs_n.shape[1] +1)])\n",
    "\n",
    "# See the data\n",
    "PCdf.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The EOFS (Empirical orthogonal functions) contain the spatial patterns associated with each PC"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "EOFs = pca.components_\n",
    "EOFs = EOFs[:n,:]\n",
    "\n",
    "# Reshape the data\n",
    "EOFs_r = EOFs.reshape((n, len(lat), len(lon)))\n",
    "EOFs_r.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nn = []\n",
    "tot_var = []\n",
    "for ip in range(n):\n",
    "    xn = pca.explained_variance_ratio_[:ip + 1].sum()\n",
    "    nn.append(xn)\n",
    "    xx =  pca.explained_variance_ratio_[:ip + 1].sum() - pca.explained_variance_ratio_[:ip ].sum()\n",
    "    tot_var.append(xx)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Convert into Xarray for visualization\n",
    "XD_EOFs_r = xr.DataArray(data=EOFs_r, coords=[(\"PCA\", tot_var), (\"lat\", lat.data), (\"lon\", lon.data)])\n",
    "\n",
    "fg = XD_EOFs_r.plot(col=\"PCA\",  col_wrap=4,\n",
    "    # The remaining kwargs customize the plot just as for not-faceted plots\n",
    "    robust=True,\n",
    "    cmap=mpl.cm.RdYlBu_r)\n",
    "\n",
    "# Use this to plot contours on each panel\n",
    "# Note that this plotting call uses the original DataArray gradients\n",
    "fg.map_dataarray(\n",
    "    xr.plot.contour, x=\"lon\", y=\"lat\", colors=\"k\", levels=13, add_colorbar=False\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-means clustering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Perform K-cluster analysis using the PCds obtained before\n",
    "nclusters = 12\n",
    "kmeans = KMeans(init='k-means++', n_clusters=nclusters, n_init=10)\n",
    "kmeans.fit(PCdf.values)\n",
    "y_pred = kmeans.fit_predict(PCdf.values)\n",
    "\n",
    "# Each day belongs to a cluster, labelled by kmeands.labels_\n",
    "np.unique(kmeans.labels_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "labels = pd.DataFrame(kmeans.labels_, index=mslp['time'], columns=['cluster'])\n",
    "\n",
    "# See how many days belong to cluster 0\n",
    "index = labels.query('cluster == {}'.format(0))\n",
    "len(index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each cluster we calculate the mean "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_tot = len(labels.cluster)\n",
    "clusters = []\n",
    "nbdays = []\n",
    "\n",
    "for iclus in range(nclusters): \n",
    "    index = labels.query('cluster == {}'.format(iclus)) \n",
    "    freq = (len(index)/num_tot)*100\n",
    "    freq = round(freq,2)\n",
    "    nbdays.append(freq)\n",
    "    cluster = mslp.sel(time=index.index).mean('time')\n",
    "    clusters.append(cluster)\n",
    "\n",
    "clusters = xr.concat(clusters, dim='cluster')\n",
    "clusters.assign_coords(cluster=nbdays)\n",
    "\n",
    "fg_C = clusters.MSL.plot(col=\"cluster\",  col_wrap=4,\n",
    "    # The remaining kwargs customize the plot just as for not-faceted plots\n",
    "    robust=True, \n",
    "    cmap=mpl.cm.RdYlBu_r)\n",
    "\n",
    "fg_C.map_dataarray(\n",
    "    xr.plot.contour, x=\"lon\", y=\"lat\", colors=\"k\", levels=13, add_colorbar=False\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Supervised learning approaches"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preparation: precipitation time series\n",
    "\n",
    "**Dataset**: RhiresD, which is a gridded daily precipitation dataset over Switzerland provided by MeteoSwiss. It is based on a spatial interpolation of rain-gauge data. The grid resolution is 1 km, but the effective resolution is in the order of 15-20 km.\n",
    "\n",
    "\n",
    "**Aggregations levels**: The gridded dataset has been averaged over different regions:\n",
    "* 12 climatic regions\n",
    "* 5 aggregated regions\n",
    "* the whole country"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Read precipitation file and get events over threshold\n",
    "precip = pd.read_csv(DATADIR + '/MeteoSwiss/precip_regions.csv')\n",
    "\n",
    "df_time = pd.to_datetime({\n",
    "    'year': precip.year,\n",
    "    'month': precip.month,\n",
    "    'day': precip.day})\n",
    "precip.insert(0, \"date\", df_time, True)\n",
    "\n",
    "precip = precip[(precip.date >= DATE_START) & (precip.date <= DATE_END)]\n",
    "\n",
    "precip_p95 = precip.copy()\n",
    "precip_p99 = precip.copy()\n",
    "\n",
    "for key, ts in precip.iteritems():\n",
    "    if key in ['year', 'month', 'day']: continue\n",
    "    precip_p95[key] = ts > ts.quantile(0.95)\n",
    "    precip_p99[key] = ts > ts.quantile(0.99)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear regression for precipitation values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Open data and get the mean value over Switzerland\n",
    "z = get_era5_data(DATADIR + '/ERA5/geopotential/*.nc', DATE_START, DATE_END)\n",
    "z500_mean = get_data_mean_over_CH_box(z, 500)\n",
    "mslp = get_era5_data(DATADIR + '/ERA5/mslp/*.nc', DATE_START, DATE_END)\n",
    "mslp_mean = get_data_mean_over_CH_box(mslp)\n",
    "t2m = get_era5_data(DATADIR + '/ERA5/Daymean_era5_T2M_EU_19790101-20210905.nc', DATE_START, DATE_END)\n",
    "t2m_mean = get_data_mean_over_CH_box(t2m)\n",
    "\n",
    "# Convert to geopotential height and hPa\n",
    "z500_mean['z'] = z500_mean['z'] / G\n",
    "mslp_mean['MSL'] = mslp_mean['MSL'] / 100"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot the time series\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(20,5))\n",
    "z500_mean.z.plot(ax=axs[0])\n",
    "axs[0].set_title('Geopotentiel 500hPa')\n",
    "mslp_mean.MSL.plot(ax=axs[1])\n",
    "axs[1].set_title('Sea level pressure')\n",
    "t2m_mean.T2MMEAN.plot(ax=axs[2])\n",
    "axs[2].set_title('Temperature 2m')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "full_set = [z500_mean, mslp_mean, t2m_mean, precip]\n",
    "\n",
    "train_set, test_set = train_test_split(precip, test_size=0.2, random_state=42)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic regression for extreme events"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random forest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Deep learning approaches"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}