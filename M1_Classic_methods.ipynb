{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Import necessary modules and do some basic setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "import seaborn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Custom utils for processing the data\n",
    "from utils_data import * \n",
    "from utils_ml import *\n",
    "from utils_plot import *\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Config matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Custom utils\n",
    "from utils_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some paths and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATADIR = os.getcwd() + '/../data'\n",
    "\n",
    "# Some constants\n",
    "DATE_START = '1979-01-01'\n",
    "DATE_END = '2020-12-31'\n",
    "YY_TRAIN = [1979, 2015]\n",
    "YY_TEST = [2016, 2020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning approaches\n",
    "\n",
    "## Getting started with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open MSLP data\n",
    "mslp = xr.open_mfdataset(DATADIR + '/ERA5/Day_era5_2deg_MSL_EU_1979-2021.nc',\n",
    "                         combine='by_coords')\n",
    "\n",
    "mslp = mslp.sel(time=slice(DATE_START, DATE_END))\n",
    "\n",
    "# Convert to hPa\n",
    "mslp.MSL.values = mslp.MSL.values/100\n",
    "lon = mslp.lon\n",
    "lat = mslp.lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an example (day) of mslp\n",
    "mslp.MSL.isel(time=200).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process season mean\n",
    "season_means = mslp.groupby(\"time.season\").mean()\n",
    "\n",
    "fg = season_means.MSL.plot(col=\"season\", col_wrap=4,\n",
    "                           robust=True, cmap=mpl.cm.RdYlBu_r)\n",
    "\n",
    "# Plot contours\n",
    "fg.map_dataarray(\n",
    "    xr.plot.contour, x=\"lon\", y=\"lat\", colors=\"k\", levels=13, add_colorbar=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute climatology (total and by season)\n",
    "climatology = mslp.mean('time')\n",
    "climatology_seasons = mslp.groupby('time.season').mean('time')\n",
    "\n",
    "# Compute anomalies (total and by season)\n",
    "anom_mslp = mslp.MSL - climatology\n",
    "anom_mslp_seasons = mslp.groupby('time.season') - climatology_seasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "Start using the whole data set for PCA. Then, anomalies can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to reshape the data [time,latxlon]\n",
    "mslp_stacked = mslp.stack(latlon=('lat', 'lon'))\n",
    "\n",
    "# Load in memory for computing the PCA\n",
    "mslp_stacked.load()\n",
    "\n",
    "# Extract msl variable\n",
    "X = mslp_stacked.MSL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of variables (features) is 1025 (41 points in longitude * 25 points in latitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise the data\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the PCA\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "nb_pc_90 = np.argmax(cumsum >= 0.90) + 1\n",
    "nb_pc_95 = np.argmax(cumsum >= 0.95) + 1\n",
    "\n",
    "print(f'Number of PC to explain 90% of the variance: {nb_pc_90}')\n",
    "print(f'Number of PC to explain 95% of the variance: {nb_pc_95}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot component contributions\n",
    "f, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(range(1,21), pca.explained_variance_ratio_[0:20]*100)\n",
    "ax.plot(range(1,21), pca.explained_variance_ratio_[0:20]*100,'ro')\n",
    "ax.grid(ls=':')\n",
    "ax.set_xticks(range(1,21))\n",
    "ax.set_xlabel('PC#')\n",
    "ax.set_ylabel(\"% variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 12 components (explaining 90% of the variance)\n",
    "nb_pc = nb_pc_90\n",
    "pca.explained_variance_ratio_[:nb_pc].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCs = pca.fit_transform(X)\n",
    "PCs_n = PCs[:, :nb_pc]\n",
    "\n",
    "# Data frame format for the selected components\n",
    "PCdf = pd.DataFrame(PCs_n, index=mslp['time'],\n",
    "                    columns=[\"PC%s\" % (x) for x in range(1, PCs_n.shape[1] + 1)])\n",
    "\n",
    "# See the data\n",
    "PCdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EOFS (Empirical orthogonal functions) contain the spatial patterns associated with each PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOFs = pca.components_\n",
    "EOFs = EOFs[:nb_pc, :]\n",
    "\n",
    "# Reshape the data\n",
    "EOFs_r = EOFs.reshape((nb_pc, len(lat), len(lon)))\n",
    "EOFs_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into Xarray for visualization\n",
    "exp_var = pca.explained_variance_ratio_[:nb_pc]\n",
    "XD_EOFs_r = xr.DataArray(data=EOFs_r, coords=[(\"PCA\", exp_var), (\"lat\", lat.data), (\"lon\", lon.data)])\n",
    "\n",
    "fg = XD_EOFs_r.plot(col=\"PCA\", col_wrap=4, robust=True, cmap=mpl.cm.RdYlBu_r)\n",
    "\n",
    "fg.map_dataarray(\n",
    "    xr.plot.contour, x=\"lon\", y=\"lat\", colors=\"k\", levels=13, add_colorbar=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-cluster analysis using the PCds obtained before\n",
    "nclusters = 12\n",
    "kmeans = KMeans(init='k-means++', n_clusters=nclusters, n_init=10)\n",
    "kmeans.fit(PCdf.values)\n",
    "y_pred = kmeans.fit_predict(PCdf.values)\n",
    "\n",
    "# Each day belongs to a cluster, labelled by kmeands.labels_\n",
    "np.unique(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(kmeans.labels_, index=np.array(mslp['time']), columns=['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how many days belong to cluster 0\n",
    "index = labels.query('cluster == {}'.format(0))\n",
    "len(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each cluster we calculate the mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tot = len(labels.cluster)\n",
    "clusters = []\n",
    "day_clusters = []\n",
    "nbdays = []\n",
    "\n",
    "for i_clus in range(nclusters):\n",
    "    index = labels.query('cluster == {}'.format(i_clus))\n",
    "    freq = (len(index)/num_tot)*100\n",
    "    freq = round(freq, 2)\n",
    "    nbdays.append(freq)\n",
    "    cluster = mslp.sel(time=index.index).mean('time')\n",
    "    d_cluster = mslp.sel(time=index.index)\n",
    "    clusters.append(cluster)\n",
    "    day_clusters.append(d_cluster)\n",
    "\n",
    "clusters = xr.concat(clusters, dim='cluster')\n",
    "clusters.assign_coords(cluster=nbdays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_c = clusters.MSL.plot(col=\"cluster\",  col_wrap=4, robust=True, cmap=mpl.cm.RdYlBu_r)\n",
    "\n",
    "fg_c.map_dataarray(\n",
    "    xr.plot.contour, x=\"lon\", y=\"lat\", colors=\"k\", levels=13, add_colorbar=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PCs and Clusters\n",
    "PCdf['date'] = PCdf.index\n",
    "PCdf.to_csv(DATADIR + 'PCdf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cluster_data(day_clusters):\n",
    "    t_list = []\n",
    "    for i_clus in range(len(day_clusters)):\n",
    "        tmp = day_clusters[i_clus].mean(dim=['lon', 'lat'])\n",
    "        tmp_df = pd.DataFrame({'date': tmp['time'], 'MSL': tmp['MSL'], 'Cluster': i_clus})\n",
    "        t_list.append(tmp_df)\n",
    "\n",
    "    # Merge by date\n",
    "    df = pd.concat(t_list)\n",
    "    df = df.sort_values(by=\"date\")\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_spatial_mean = prepare_cluster_data(day_clusters)\n",
    "cluster_spatial_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_spatial_mean.to_csv(DATADIR + 'Cluster_spatialmean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "## Target: precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prec = get_precipitation_data(DATADIR + '/MeteoSwiss/precip_regions.csv', DATE_START, DATE_END)\n",
    "\n",
    "# Select the right columns: date and reg_tot (whole country)\n",
    "df_prec = df_prec[['date', 'reg_tot']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input variables\n",
    "A set of primary meteorological variables are used as covariates.\n",
    "- Geopotential levels (1000, 850, 700, 500, 300)\n",
    "- MSL: Mean sea level pressure\n",
    "- T2MMEAN: 2m-temperature\n",
    "\n",
    "**Extra-variables such as lagged T2MMEAN and PCs will be used (see below)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "l_files = glob.glob(os.path.join(DATADIR, 'ERA5', 'TS_CH', 'df*.csv'))\n",
    "df_vars = read_csv_files(l_files, DATE_START, DATE_END)\n",
    "df_vars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lag-t2mmean: For precipiation the memory system is important, adding lagged variables might help to predict precipitation\n",
    "df_vars['T2MLag'] = df_vars['T2MMEAN'].shift(1)\n",
    "df_vars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use PCs from the previous steps (PCA-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Large scale-atmospheric PCs and Clusters\n",
    "df_PCs = pd.read_csv(DATADIR + '/ERA5/PCdf.csv')\n",
    "df_PCs['date'] = pd.DatetimeIndex(df_PCs['date']).normalize()\n",
    "df_clusters = pd.read_csv(DATADIR + '/ERA5/Cluster_spatialmean.csv')\n",
    "df_clusters = df_clusters[['date', 'Cluster']]\n",
    "df_clusters['date'] = pd.DatetimeIndex(df_clusters['date']).normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge everything\n",
    "l_all = [df_vars, df_prec]\n",
    "df_all = concat_dataframes(l_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "seaborn.pairplot(df_all, vars=df_all.columns[1:9],\n",
    "                 kind='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylabel = df_prec.columns[1]\n",
    "\n",
    "# Add categorical variables\n",
    "df_input = pd.merge(df_all, df_PCs)\n",
    "names_col = df_input.columns\n",
    "\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the target variable: precipitation\n",
    "df_input['reg_tot'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define attributes - i.e covariates: remove MSL as using the PCs\n",
    "attributes = names_col.drop(['date','reg_tot','MSL'])\n",
    "\n",
    "# Split data into training and test\n",
    "train_dataset, train_labels, test_dataset, test_labels, train_dates, test_dates = split_data(df_input, YY_TRAIN, YY_TEST, attributes, ylabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of using the clusters, it has to be a categorical variable: cat_var='Cluster'\n",
    "# In the following we won't use categorical variables, as we're using the PCs\n",
    "fpipeline = create_pipeline(train_dataset, None)\n",
    "X_prep_train = fpipeline.fit_transform(train_dataset)\n",
    "X_prep_test = fpipeline.transform(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prep_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multiple Linear regression for precipitation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(n_jobs=16)\n",
    "lr.fit(X_prep_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train = mean_squared_error(train_labels, lr.predict(X_prep_train))\n",
    "mse_test = mean_squared_error(test_labels, lr.predict(X_prep_test))\n",
    "\n",
    "print(f'Train MSE = {mse_train}')\n",
    "print(f'Test MSE = {mse_test}')\n",
    "print(f'Train RMSE = {np.sqrt(mse_train)}')\n",
    "print(f'Test RMSE = {np.sqrt(mse_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(lr)             \n",
    "rfe = rfe.fit(X_prep_train, train_labels)\n",
    "mean_squared_error(train_labels, rfe.predict(X_prep_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_val_predict returns an array of the same size as `y` where each entry\n",
    "# is a prediction obtained by cross validated:\n",
    "lr_cv_mse = cross_val_score(lr, X_prep_train, train_labels, scoring='neg_mean_squared_error', cv=10)\n",
    "\n",
    "# We got the negative average MSE for cross-validation (minimizing MSE is equivalent to maximizing the negative MSE)\n",
    "lr_cv_mse.mean()\n",
    "# The result is close to what we obtained before. The negative result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_rmse_scores = np.sqrt(-lr_cv_mse)\n",
    "pd.Series(lin_rmse_scores).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Folds: {len(lr_cv_mse)}, MSE: {np.mean(np.abs(lr_cv_mse))}, STD: {np.std(lr_cv_mse)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients\n",
    "lr.coef_\n",
    "# coeff_df = pd.DataFrame(lr.coef_, attributes, columns=['Coefficient'])\n",
    "# makes some predictions\n",
    "y_pred = lr.predict(X_prep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_ts(test_dates, y_pred, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we're getting negative predicted values, let's try to transform the target variable \n",
    "# Transform targets and use same linear model\n",
    "regr_trans = TransformedTargetRegressor(regressor=LinearRegression(),\n",
    "                                        func=np.log1p,\n",
    "                                        inverse_func=np.expm1)\n",
    "\n",
    "regr_trans.fit(X_prep_train, train_labels)\n",
    "y_trans_pred = regr_trans.predict(X_prep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_transf_train = mean_squared_error(train_labels, regr_trans.predict(X_prep_train))\n",
    "mse_transf_test = mean_squared_error(test_labels, regr_trans.predict(X_prep_test))\n",
    "print(f'Train MSE = {mse_transf_train}'); print(f'Test MSE = {mse_transf_test}')\n",
    "print(f'Train RMSE = {np.sqrt(mse_transf_train)}'); print(f'Test RMSE = {np.sqrt(mse_transf_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "plot_prediction_ts(test_dates, y_trans_pred, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The predictions seem to be better, but the MSE are still large*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "forest_reg.fit(X_prep_train, train_labels)\n",
    "\n",
    "# Make predictions\n",
    "y_rf_pred = forest_reg.predict(X_prep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_rf_train = mean_squared_error(train_labels, forest_reg.predict(X_prep_train))\n",
    "mse_rf_test = mean_squared_error(test_labels, forest_reg.predict(X_prep_test))\n",
    "print(f'Train MSE = {mse_rf_train}'); print(f'Test MSE = {mse_rf_test}')\n",
    "print(f'Train RMSE = {np.sqrt(mse_rf_train)}'); print(f'Test RMSE = {np.sqrt(mse_rf_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see overfitting problem...\n",
    "\n",
    "### Tuning parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search\n",
    "param_grid = [\n",
    "    # try 12 (3×4) combinations of hyperparameters\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6]},\n",
    "    # then try 6 (2×3) combinations with bootstrap set as False\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Train across 5 folds, that's a total of (12+6)*5=90 rounds of training\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(X_prep_train, train_labels)\n",
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "print(\"Best params:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_GCV_reg = RandomForestRegressor(n_jobs=-1).set_params(**best_params)\n",
    "forest_GCV_reg.fit(X_prep_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rf_cv_predict = forest_GCV_reg.predict(X_prep_test)\n",
    "mse_rf_cv_train = mean_squared_error(train_labels, forest_GCV_reg.predict(X_prep_train))\n",
    "mse_rf_cv_test = mean_squared_error(test_labels, forest_GCV_reg.predict(X_prep_test))\n",
    "print(f'Train MSE = {mse_rf_cv_train}')\n",
    "print(f'Test MSE = {mse_rf_cv_test}')\n",
    "print(f'Train RMSE = {np.sqrt(mse_rf_cv_train)}')\n",
    "print(f'Test RMSE = {np.sqrt(mse_rf_cv_test)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_ts(test_dates, y_rf_cv_predict, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importance = forest_GCV_reg.feature_importances_\n",
    "sorted_features_importance = sorted(zip(features_importance, attributes), reverse=True)\n",
    "sorted_features_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(features_importance)\n",
    "plt.barh(range(len(attributes)), features_importance[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [attributes[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting extremes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define exceedances based on the 95th\n",
    "df_prec_ex = precip_exceedance(df_prec)\n",
    "df_prec_ex['reg_tot'] = df_prec_ex['reg_tot']*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_ex = df_input\n",
    "\n",
    "# Replace reg_tot by the exceedances\n",
    "df_input_ex['reg_tot'] = df_prec_ex['reg_tot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_labels, test_dataset, test_labels, train_dates, test_dates = split_data(df_input_ex, YY_TRAIN, YY_TEST, attributes, ylabel)\n",
    "# but the data is already in the format (only the labels have been replace by the exceedances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All parameters not specified are set to their defaults\n",
    "logistic_reg = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "logistic_reg.fit(X_prep_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ex_pred = logistic_reg.predict(X_prep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(test_labels, y_ex_pred)\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use score method to get accuracy of model\n",
    "score = logistic_reg.score(X_prep_test, test_labels)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, y_ex_pred, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make probability predictions\n",
    "train_probs = logistic_reg.predict_proba(X_prep_train)[:, 1]\n",
    "probs = logistic_reg.predict_proba(X_prep_test)[:, 1]\n",
    "\n",
    "train_predictions = logistic_reg.predict(X_prep_train)\n",
    "predictions = logistic_reg.predict(X_prep_test)\n",
    "print(f'Train ROC AUC Score: {roc_auc_score(train_labels, train_probs)}')\n",
    "print(f'Test ROC AUC  Score: {roc_auc_score(test_labels, probs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(test_labels, train_labels, predictions,\n",
    "               probs, train_predictions, train_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with 100 trees\n",
    "rf_class = RandomForestClassifier(n_estimators=150,\n",
    "                                  random_state=42,\n",
    "                                  max_features='sqrt',\n",
    "                                  n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit on training data\n",
    "rf_class.fit(X_prep_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = []\n",
    "max_depths = []\n",
    "\n",
    "for ind_tree in rf_class.estimators_:\n",
    "    n_nodes.append(ind_tree.tree_.node_count)\n",
    "    max_depths.append(ind_tree.tree_.max_depth)\n",
    "\n",
    "print(f'Average number of nodes {int(np.mean(n_nodes))}')\n",
    "print(f'Average maximum depth {int(np.mean(max_depths))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rf_predictions = rf_class.predict(X_prep_train)\n",
    "train_rf_probs = rf_class.predict_proba(X_prep_train)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions = rf_class.predict(X_prep_test)\n",
    "rf_probs = rf_class.predict_proba(X_prep_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(test_labels, train_labels, rf_predictions,\n",
    "               rf_probs, train_rf_predictions, train_rf_probs)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
